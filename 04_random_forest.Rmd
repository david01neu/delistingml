## Random forest

**Feature names for random forest model**

```{r, model.random_forest.features}
pander(names(data.training %>% select(-permno, -public_date)))
```

The random forest implementation `randomForestSRC` provides three
hyperparameters.

* *ntree*: number of trees (default: 500)

* *nodesize*: minimum size of terminal node (default: 5)

* *mtry*: number of variables to possibly split at each node (default:
number of variables divided by 3)


**Find optimal mtry and nodesize tuning parameter using standardized out-of-sample
error**
```{r, model.random_forest.tune}
if (REDO_MODEL_FITTING) {
  set.seed(1)
  # finds optimal mtry and nodesize tuning parameter for a random forest using
  # standardized out-of-sample-error
  rfr.reg.1000.tuned <- tune(
    response ~ .,
    as.data.frame(data.training %>% 
                    mutate(response = ifelse(response == "delisted", 1, 0)) %>% 
                    select(-permno, -public_date)),
    mtryStart = sqrt(ncol(data.training) - 3), # number of covariates used for each tree
    nodesizeTry = c(1:9, seq(10, 100, by = 5)), # minimum size of terminal node
    ntreeTry = 1000,
    sampsize = function(x) {
        min(x * .632, max(150, x^(3 / 4)))
      }, # sampling without replacement
    nsplit = 10,
    stepFactor = 1.25, # at each iteration, mtry is inflated by this value
    improve = 1e-3,
    strikeout = 5,
    maxIter = 25,
    trace = FALSE,
    doBest = TRUE
  )
  save(rfr.reg.1000.tuned, file = "../data/rfr.reg.1000.tuned.RData")
} else {
  load("../data/rfr.reg.1000.tuned.RData")
}

# contour plot of standardized out-of-sample errors for different nodesize and mtry
plot.tune <- function(o, linear = TRUE) {
  nodesize <- o$results[, 1]
  mtry <- o$results[, 2]
  oob_error <- o$results[, 3]
  # so <- interp(x=x, y=y, z=z, linear = linear)
  idx <- which.min(oob_error)
  x0 <- nodesize[idx]
  y0 <- mtry[idx]

  ggplot(data.frame(nodesize = nodesize, mtry = mtry, oob_error = oob_error)) +
    geom_contour_filled(mapping = aes(nodesize, mtry, z = oob_error)) +
    labs(
      title = "Error rate for nodesize and mtry",
      subtitle = "OOB errors"
    )
}
plot.tune(rfr.reg.1000.tuned)
```

The contour plot shows a standardized out-of-sample error for random
forests with 1000 trees and different *nodesize* and *mtry*. The lowest
errors are observed for small *nodesizes* and large *mtry*.

**Metrics for varying nodesize and mtry**
```{r, model.random_forest.metrics_evaluation}
optimization.nodesize.mtry <- tibble(
  nodesize = numeric(),
  mtry = numeric(),
  samples = character(),
  tp = numeric(),
  fn = numeric(),
  fp = numeric(),
  tn = numeric(),
  accuracy = numeric(),
  precision = numeric(),
  recall = numeric(),
  fscore = numeric()
)

if (REDO_MODEL_FITTING) {
  for (nodesize in c(1:14, seq(15, 100, by = 5))) {
    for (mtry in seq(ceiling(sqrt(ncol(data.training))), ncol(data.training)*0.7, by = 5)) {
      print(paste("nodesize:", nodesize, "| mtry:", mtry))
      rfr.nodesize.mtry.result <- rfsrc(
        response ~ .,
        data = as.data.frame(data.training %>% 
                    mutate(response = ifelse(response == "delisted", 1, 0)) %>% 
                    select(-permno, -public_date)),
        nodesize = nodesize,
        ntree = 1000,
        mtry = mtry,
        statistics = TRUE,
        save.memory = TRUE
      )
      
      train_prediction_table <- tibble(
        actual = (data.training %>% mutate(response = ifelse(response == "delisted", 1, 0)))$response,
        predicted.inb = rfr.nodesize.mtry.result$predicted,
        predicted.oob = rfr.nodesize.mtry.result$predicted.oob
      )
      auc_roc.result <- auc_roc(train_prediction_table$predicted.inb,
                                train_prediction_table$actual, returnDT=TRUE)
      
      # best classification threshold
      best_cutpoint <- auc_roc.result %>% #######################################
        # Pythagoras (distance to (0,1))
        mutate(dist = sqrt((1 - CumulativeTPR)^2 + CumulativeFPR^2)) %>%
        filter(dist <= min(dist))
      
      train_prediction_table <- train_prediction_table %>% mutate(
          result.inb = predicted.inb > best_cutpoint$Pred,
          result.oob = predicted.oob > best_cutpoint$Pred
        )
      
      optimization.nodesize.mtry <- rbind(
        optimization.nodesize.mtry, append(
        compute.metrics(train_prediction_table$actual == 1,
        train_prediction_table$result.inb),
        list(nodesize = nodesize, mtry = mtry, samples = 0), after = 0))
      
      optimization.nodesize.mtry <- rbind(optimization.nodesize.mtry, append(
        compute.metrics(train_prediction_table$actual == 1,
        train_prediction_table$result.oob),
        list(nodesize = nodesize, mtry = mtry, samples = 1), after = 0))
    }
  }
  colnames(optimization.nodesize.mtry) <- c("nodesize", "mtry", "samples", 
                                            "TP", "FN", "FP", "TN", "accuracy", 
                                            "precision", "recall", "f1", 
                                            "specificity", "mcc")
  # convert binary variable to INB and OOB factors
  optimization.nodesize.mtry <- optimization.nodesize.mtry %>% 
    mutate(samples = factor(samples,
                            levels = c(0, 1),
                            labels = c("INB", "OOB")))
  save(optimization.nodesize.mtry, file = "../data/optimization.nodesize.mtry.RData")
} else {
  load("../data/optimization.nodesize.mtry.RData")
}

plot.nodesize.metric = function (data, metric, mtry.fix) {
    plot <- ggplot(data = data %>% filter(mtry == mtry.fix),
                   aes(x = nodesize, col = samples, linetype = samples)) +
      geom_line(aes_string(y = metric), linewidth = 1) +
      labs(
        title = paste("Nodesize vs", metric),
        subtitle = paste("RF: ntree = 1000 and mtry =", mtry.fix),
        x = "nodesize",
        y = metric,
        colour = "Evaluation type",
        linetype = "Evaluation type"
      )
}

plot.mtry.metric <- function (data, metric, nodesize.fix) {
    plot <- ggplot(data = data %>% filter(nodesize == nodesize.fix), 
                   aes(x = mtry, col = samples, linetype = samples)) +
      geom_line(aes_string(y = metric), linewidth = 1) +
      labs(
        title = paste("Mtry vs", metric),
        subtitle = paste("RF: ntree = 1000 and nodesize =", nodesize.fix),
        x = "mtry",
        y = metric,
        colour = "Evaluation type",
        linetype = "Evaluation type"
      )
}

metrics <- c("accuracy", "precision", "recall", "f1")
plot.nodesize.metrics.list <- lapply(metrics, plot.nodesize.metric,
                            data = optimization.nodesize.mtry, mtry.fix = 17)
plot.mtry.metrics.list <- lapply(metrics, plot.mtry.metric,
                            data = optimization.nodesize.mtry, nodesize.fix = 15)

plot.nodesize.metrics <- ggarrange(plot.nodesize.metrics.list[[1]], plot.nodesize.metrics.list[[2]], 
          plot.nodesize.metrics.list[[3]], plot.nodesize.metrics.list[[4]],
                  nrow = 2,
                  ncol = 2,
                  legend = "bottom",
                  common.legend = TRUE)
plot.mtry.metrics <- ggarrange(plot.nodesize.metrics.list[[1]], plot.nodesize.metrics.list[[2]], 
          plot.nodesize.metrics.list[[3]], plot.nodesize.metrics.list[[4]], plot.mtry.metrics.list[[1]], plot.mtry.metrics.list[[2]], 
          plot.mtry.metrics.list[[3]], plot.mtry.metrics.list[[4]],
                  nrow = 2,
                  ncol = 4,
                  legend = "bottom",
                  common.legend = TRUE)
plot.mtry.metrics
```

Parameter choice for random forest

* `ntree`: 1000
* `nodesize`: 5
* `mtry`: 12

```{r, model.random_forest.caret}
# function and object definitions for custom model in caret package
# https://topepo.github.io/caret/using-your-own-model-in-train.html
rf.parameters <- data.frame(
  parameter = c("ntree", "nodesize", "mtry"),
  class = rep("numeric", 3)
)

rf.grid <- function(x, y, len = NULL, search = "grid") { }

rf.fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) { 
    rfr.result <- rfsrc(
      y ~ .,
      data = data.frame(x, y = as.factor(y)),
      ntree = param$ntree,
      nodesize = param$nodesize,
      mtry = param$mtry,
      statistics = TRUE,
      save.memory = TRUE
    )
    
    return (rfr.result)
}

rf.predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  prediction <- randomForestSRC::predict.rfsrc(modelFit, as.data.frame(newdata))$class
  return (prediction)
}

rf.predict.prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL) {
  prediction <- randomForestSRC::predict.rfsrc(modelFit, as.data.frame(newdata))$predicted
  return (prediction)#data.frame(active = 1 - prediction, delisted = prediction))
}

rf.list = list(
  type = c("Classification", "Regression"),
  method = "randomForestSRC",
  library = c("randomForestSRC"),
  parameters = rf.parameters,
  grid = rf.grid,
  fit = rf.fit,
  predict = rf.predict,
  prob = rf.predict.prob,
  sort = function(x) x
)
```

**Model fit**
```{r, model.random_forest.fit}
if (REDO_MODEL_FITTING) {
  ### control the computational nuances of the train function ####################
  rf.fitControl <- trainControl(method = "cv",
                               number = 10,
                               ## Estimate class probabilities
                               classProbs = TRUE,
                               ## Evaluate performance using 
                               ## the following function
                               summaryFunction = compute.summary,
                               returnResamp = "all")
  
  ### fit model over different tuning parameters #################################
  set.seed(1)
  rf.fit <- caret::train(response ~ ., data = data.training %>% select(-permno, -public_date), 
                   preProcess = c("center", "scale"),
                   method = rf.list, 
                   trControl = rf.fitControl,
                   tuneGrid = expand.grid(
                     ntree = 1000,
                     nodesize = 5,
                     mtry = 12),
                   verbose = TRUE, 
                   metric = "mcc.mccf1" # specify which metric to optimize
                   ) 
  save(rf.fit, file = "../data/rf.fit.RData")
} else {
  load("../data/rf.fit.RData")
}
```

**Model information and evaluation on training set**
```{r, model.random_forest.best_model}
rf.bestTune.mccf1 <- rf.fit$resample %>%
  group_by(ntree, nodesize, mtry) %>%
  summarize(mccf1 = mean(mcc.mccf1), .groups = "drop") %>%
  slice(which.max(mccf1))

rf.bestTune.roc <- rf.fit$resample %>%
  group_by(ntree, nodesize, mtry) %>%
  summarize(auc = mean(auc.roc), .groups = "drop") %>%
  slice(which.max(auc))

# Hyperparameters of best models for MCCF1 and ROC cutpoints
hyperparameters.table <- tibble(
  cutpoint = c("MCCF1", "ROC"),
  ntree = c(rf.bestTune.mccf1$ntree, rf.bestTune.roc$ntree),
  nodesize = c(rf.bestTune.mccf1$nodesize, rf.bestTune.roc$nodesize),
  mtry = c(rf.bestTune.mccf1$mtry, rf.bestTune.roc$mtry)
)

pred.probability <- predict(rf.fit, newdata = data.training, type = "prob")
rf.metrics <- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted)

illustrate.metrics.boxplot(rf.fit$resample %>% filter(ntree == hyperparameters.table$ntree[1] &
                                                      nodesize == hyperparameters.table$nodesize[1] &
                                                      mtry == hyperparameters.table$mtry[1]),
                           rf.fit$resample %>% filter(ntree == hyperparameters.table$ntree[2] &
                                                      nodesize == hyperparameters.table$nodesize[2] &
                                                      mtry == hyperparameters.table$mtry[2]),
                           "rf")
```

**Performance evaluation on test set**
```{r, model.random_forest.test}
# prediction scores on test data: ("active", "delisted")
pred.test.probability <- predict(rf.fit, newdata = data.test, type = "prob")

# metrics on test set using best threshold determined based on train data
rf.summary.test <- rbind(compute.metrics(data.test$response, pred.test.probability$delisted >= rf.metrics$cutpoint.roc),
      compute.metrics(data.test$response, pred.test.probability$delisted >= rf.metrics$cutpoint.mccf1)) %>%
  cbind(splitting = c("ROC", "MCCF1"), .)
pander(rf.summary.test, caption = "Random forest on test data")
```

**Variable importance**

**Definition [Permutation (Breiman-Cutler)
Importance](https://www.randomforestsrc.org/articles/rfsrc-subsample.html)**:
In the OOB cases for a tree, randomly permute all values of the j-th
variable. Put these new covariate values down the tree and compute a new
internal error rate. The amount by which this new error exceeds the
original OOB error is defined as the importance of the j-th variable for
the tree. Averaging over the forest yields variable importance (VIMP).

The following chunk determines the variable importance of all the
features in the model data set using the permutation importance method.
The twenty features with the highest variable importance are displayed
in the barplot.

```{r, random_forest.vimp}
rfr.vimp.results <- vimp(rf.fit$finalModel, importance = "permute")
rfr.vimp <- tibble(
  feature = rfr.vimp.results$xvar.names,
  vimp = 100 * rfr.vimp.results$importance[,"all"]
) %>% arrange(desc(vimp))

plot.vimp(rfr.vimp, "Permutation (Beiman-Cutler) Importance")
```

The price-to-earnings ratio feature `pe_exi` is the most important financial ratio feature. The histogram with delisting frequency for `pe_exi` indicates indeed high predictive power between the
`pe_exi` values near zero and the relative delisting frequency.\
The financial ratio features `aftret_equity`, `opmad` and `roe`` follow in the importance ranking. This is plausible as the histograms with delisting frequency (see Exploratory Data Analysis section) indicated predictive power for delisting. If the (after-tax) return on equity is low and/or the operative profit margin is negative, then the delisting frequency is relatively high.

**Marginal plots**

**Definition [Partial Dependence
Function](https://www.randomforestsrc.org/articles/partial.html)**: Let
$F(x)$ be the target function in a supervised problem where
$x = (x_1, \dots, x_p)$. Let $x_s$ denote $x$ restricted to coordinate
indices $s \subset \{1,\dots,p\}$. Likewise using the notation
$\backslash s=\{1,…,p\}\backslash s$ to denote the complement of $s$,
let $x∖backslash s$ denote the coordinates of $x$ with indices not in
$s$. The (marginal) partial dependence function is $$
\overline{F}(x_s) = \int F(x_s,x_{\backslash s}) \cdot p_{\backslash s}(x_{\backslash s})dx_{\backslash s}
$$ where
$p_{\backslash s}(x_{\backslash s}) = \int p(x) dx_{\backslash s}$ is
the marginal probability density of $x_{\backslash s}$.

The target function $F(x)$ is usually not known for supervised learning
problems. In a random forest, the target function is estimated by an
ensemble of trees. Let $\hat{F}$ denote this estimator. Let
$X_1,\dots, X_n$ denote the features from the learning data, then the
estimated partial dependence function is $$
\hat{\overline{F}}(x_s) = \frac{1}{n} \sum_{i=1}^n \hat{F}(x_s, X_{i,\backslash s})
$$ The plots of the estimated marginal partial dependence function for
the twenty most important features are displayed below.

```{r, model.random_forest.partial_plots}
if (REDO_MODEL_FITTING) {
  rf.partial_plots = list()
  counter = 1
  for (feature_identifier in (rfr.vimp %>% filter(row_number() <= 20))$feature) {
    min <- min(rf.fit$finalModel$xvar[feature_identifier])
    max <- max(rf.fit$finalModel$xvar[feature_identifier])
    sequence_values <- seq(min, max, length.out = 30)
    
    # partial effect for feature feature_identifier
    partial.obj <- partial(rf.fit$finalModel,
      partial.xvar = feature_identifier,
      partial.values = sequence_values
    )

    ## helper function for extracting the partial effects
    pdta <- get.partial.plot.data(partial.obj)
    
    rf.partial_plots[[counter]] <- tibble(x = pdta$x, y = pdta$yhat)
    counter = counter + 1
  }
  save(rf.partial_plots, file = "../data/rf.partial_plots.RData")
} else {
  load("../data/rf.partial_plots.RData")
}

counter = 1
# marginal plots of 20 most important features on test data
for (feature_identifier in (rfr.vimp %>% filter(row_number() <= 20))$feature) {
  plot <- hist_delisting_frequency(
    (data.test %>%
        inner_join(com %>% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))),
    feature_identifier,
    feature_identifier,
    marginal.data = pred.test.probability$delisted
  )
  print(plot)
  
  # marginal plots of 20 most important features and feature pe_exi on test data
  plot <- ggplot(
      data = rf.partial_plots[[counter]],
      aes(x = x, y = y)
    ) +
    geom_point() +
    geom_smooth() +
    labs(
      title = "Partial dependence plot",
      subtitle = "Random forest",
      x = feature_identifier,
      y = "Prediction"
    )

  print(plot)
  counter = counter + 1
}
```

-   `pe_exi1` and `pe_exi2`: If the price-to-earnings ratio (per share)
    is around zero (market value per share around zero), more delistings occur than with an in absolute terms larger price-to-earnings ratio. If the PE ratio is negative, the market value of a share is large compared to a small negative loss. If the PE ratio is positive, the market value per share is lower than the earnings per share which signifies underpricing of the share.\

-   `cash_ratio1`, `quick_ratio1`: If the ratio between cash (plus
    short-term investments) and liabilities increases, the partial
    dependence function decreases.\

-   `opmbd1`, `opmbd2`: The observed and modeled delisting frequency is
    relatively high for negative operative profit margins. The frequency
    drops the more the operative profit margin gets positive.\

-   `roe1`: If the returns on equity are negative, the observed and
    modeled delisting frequency is higher thant for positive values.

-   `aftret_equity`: If the net income over total shareholder's equity
    is negative, the observed and modeled delisting frequency is higher
    than for positive ratios.\

-   `fredfund_chg`: The the Federal Reserve decreases the federal funds
    rate in periods of economic crisis. In these times, relatively many
    companies delist. In times of good conjuncture, the Federal Reserve
    rises interest rates. **(Will this result in problems for the years
    2021-2023?)**\

-   `roe_h`: The random forest models a higher delisting frequency for
    companies whose return on equity decreased than for companies whose
    return on equity increased in the past up to seven years.\

-   `at_turn_h`: If the asset turnover declined over the past seven
    years\

-   `cpiaucsl`: The modeled delisting frequency is the highest for negative or small positive CPI rate changes. There is a decrease followed by a global minimum in modeled delisting between an annual inflation rate of 0.06 and 0.09. For CPI rate changes higher than 0.09, the modeled delisting frequency increases.\

-   `cash_ratio_h`, `quick_ratio_h`: If the cash ratio increased in the
    past, the modeled delisting frequency is lower than for a decrease
    of the ratio in the past.\

-   `aftret_equity_h`, `aftret_eq_h`: If the net income to equity ratio
    increased in the past, the modeled delisting frequency is lower than
    for a decrease of the ratio in the past.\

-   `roe_h`, `roa_h`: ?????????????????\

- `debt_assets_h`: ????????????\

- `pay_turn_h`: Cost of goods sold / Average of Accout payables

- `pe_exi_h`: If the PE ratio increased in the past, the

- `gdpc1_chg`: If the gross domestic product change is large, delisting is relatively high.
-   *R packages for neural networks, SVN?*

Further models possibly coming soon: Logistic Regression with Lasso
regularization, Neural Network, SVM, Boosting




