[["index.html", "Prediction of Delisting using a Machine Learning Ensemble 1 Introduction", " Prediction of Delisting using a Machine Learning Ensemble David Neuh√§usler, Jungyeon Yoon, Richard A. Levine, Juanjuan Fan February 2024 1 Introduction library(plyr) # tools for splitting, applying and combining data library(tidyverse) # collection of R packages designed for data science library(patchwork) # simple layouts for ggplot2 library(ggpubr) # customize ggplot2 plots library(ggtext) # simple Markdown and HTML rendering for ggplot2 library(glue) # R expression embedding for strings library(formattable) # formatter functions library(latex2exp) # use LaTeX in plots library(pander) # rendering of R objects into markdown library(lubridate) # dates and times library(rsample) # functions for resampling library(mice) # missing data imputation models library(mltools) # machine learning helper functions library(parallel) # parallel processing for randomForestSRC library(randomForestSRC) # random forest implementation library(glmnet) # generalized linear models with penalization terms library(kernlab) # kernel-based machine learning methods library(gbm) # generalized boosted regression models library(devtools) # required for reticulate library(reticulate) # embeds Python session within R session library(tensorflow) # open-source machine learning platfrom # install_tensorflow() library(keras) # high-level API to build and train deep learning models # install_keras() library(caret) # misc functions for training and plotting ml models library(caretEnsemble) # create ensemble of caret models library(testthat) # testing framework for R library(testrmd) # testing output for Rmd-files theme_set(theme_minimal() %+replace% theme( # plot title plot.title = element_text( size = rel(1.3), face = &quot;bold&quot;, margin = margin(0, 0, 5, 0), hjust = 0 ), # plot panel panel.grid.minor = element_blank(), panel.border = element_blank(), # axes axis.title = element_text(size = rel(0.85), face = &quot;bold&quot;), axis.text = element_text(size = rel(0.70), face = &quot;bold&quot;), axis.line = element_line(color = &quot;black&quot;), # legend legend.title = element_text(size = rel(0.85), face = &quot;bold&quot;), legend.text = element_text(size = rel(0.70), face = &quot;bold&quot;), legend.key = element_rect(fill = &quot;transparent&quot;, colour = NA), legend.key.size = unit(1.5, &quot;lines&quot;), legend.background = element_rect(fill = &quot;transparent&quot;, colour = NA), # plot facets strip.background = element_rect(fill = &quot;black&quot;, color = &quot;black&quot;), strip.text = element_text( size = rel(0.85), face = &quot;bold&quot;, color = &quot;white&quot;, margin = margin(5, 0, 5, 0) ) )) colors &lt;- c( &quot;#000000&quot;, &quot;#555555&quot;, &quot;#999999&quot;, &quot;#0072B2&quot;, &quot;#009E73&quot;, &quot;#E69F00&quot;, &quot;#D55E00&quot;, &quot;#F0E442&quot;, &quot;#CC79A7&quot;, &quot;#56B4E9&quot; ) names(colors) &lt;- c( &quot;black&quot;, &quot;gray&quot;, &quot;lightgray&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;yellow&quot;, &quot;magenta&quot;, &quot;lightblue&quot; ) testrmd::init() # randomForestSRC: OpenMP uses all available cores options(rf.cores = detectCores(), mc.cores = detectCores()) set.seed(1) # data constants REBUILD_INITIAL_DATA_SET &lt;- FALSE REBUILD_PREDICTOR_DATA_SET &lt;- FALSE IS_QUARTERLY_DATA &lt;- TRUE FIRST_TRADING_DATE_AVAILABLE &lt;- ymd(&quot;1970-01-01&quot;) LAST_TRADING_DATE_AVAILABLE &lt;- ymd(&quot;2022-12-30&quot;) NUMBER_YEARS_AVAILABLE &lt;- interval(FIRST_TRADING_DATE_AVAILABLE, LAST_TRADING_DATE_AVAILABLE) / years(1) MAX_DATE &lt;- ymd(&quot;2999-12-30&quot;) # constants for data preparation CORRELATION_THRESHOLD &lt;- 0.75 N_QUARTERS_GAP_TO_DELISTING &lt;- 1 N_QUARTERS_FIN_RATIOS &lt;- 4 N_QUARTERS_FIN_RATIOS_REQUIRED &lt;- N_QUARTERS_FIN_RATIOS + N_QUARTERS_GAP_TO_DELISTING N_YEARS_FIN_RATIO_HISTORY &lt;- 7 # imputation constants REDO_MICE_IMPUTATION_PPM &lt;- FALSE # model constants REDO_MODEL_FITTING &lt;- FALSE # returns TRUE if years to delisting is betwwen 0.25 and 1.25 determine_delisting_property &lt;- function(years_to_delisting) { return(0.25 &lt;= years_to_delisting &amp; years_to_delisting &lt; 1.25) } "],["data.html", "2 Data 2.1 Delisting, financial and macroeconomic data 2.2 Exploratory Data Analysis 2.3 Predictor data set", " 2 Data 2.1 Delisting, financial and macroeconomic data The data sources for this code chunk are the two databases of the Center for Research in Security Prices (CRSP) and Wharton Research Data Services (WRDS). The code snippet creates a data frame containing delisting and quarterly financial information for securities. com_ratio is the name of the resulting data frame. if (REBUILD_INITIAL_DATA_SET) { # Author: Jungyeon Yoon #----------------------------------------------- # [delist file] : CRSP - Annual Update - Stock / Events - Delist # variables : permno, dlstdt, dlstcd, nwperm, nwcomp, cusip # - permno double CRSP Permanent Issue Number (permno) # - dlstdt date Delisting Date (dlstdt) # - dlstcd double Delisting Code (dlstcd) # - nwperm double New CRSP Permno (nwperm) # - nwcomp double New CRSP Permco (nwcomp) # [com_info file] : CRSP - Annual Update - Stock / Security Files - Stock Header Info # variables : permno, permco, hsiccd, hshrcd, hcomnam, begdat, enddat # - permno double CRSP Permanent Issue Number (permno) # - permco double CRSP Permanent Company Number (permco) # - hsiccd double Header SIC Code (hsiccd) # - HSHRCD double Share Code Header (HSHRCD) # - HCOMNAM string Company Name Header (HCOMNAM) # - BEGDAT date Begin of Stock Data (BEGDAT) # - ENDDAT date End of Stock Data (ENDDAT) #------------------------------------------------ # loading saved delist and com_info datasets load(&quot;../data/delist.RData&quot;) # delist : permno # dim(delist) # 36945 6 load(file = &quot;../data/com_info.RData&quot;) # com_info : permno permco # dim(com_info) # 36974 7 #---------------------------- # combine delist and com_info and make one combined dataset(comp) comp0 &lt;- com_info %&gt;% left_join(delist, by = c(&quot;permno&quot; = &quot;permno&quot;)) # data ranges range(comp0$begdat) # &quot;1925-12-31&quot; &quot;2022-12-30&quot; range(comp0$enddat) # &quot;1926-02-24&quot; &quot;2022-12-30&quot; # remove obs with Delisting Date (dlstdt) is missing comp &lt;- comp0 %&gt;% filter(!is.na(dlstdt)) # nrow(comp) # 29 observations removed comp &lt;- comp %&gt;% mutate(days = enddat - begdat) # subset companies that listed from 1970 start_d &lt;- as.Date(&quot;1970-01-01&quot;) end_d &lt;- as.Date(&quot;2022-12-30&quot;) comp &lt;- comp %&gt;% filter(days &gt; 0 &amp; begdat &gt;= start_d) nrow(comp) # 33336 range(comp$begdat) # &quot;1970-01-06&quot; &quot;2022-12-29&quot; range(comp$enddat) # &quot;1970-12-29&quot; &quot;2022-12-30&quot; # delisting code as group range(comp$dlstcd) # 100-591 # categorize delisting group (Active, Mergers, Exchanges, Liquidation, Dropped) comp &lt;- comp %&gt;% mutate(event = substr(dlstcd, 1, 1)) %&gt;% mutate(event_group = factor(event, label = c(&quot;active&quot;, &quot;mergers&quot;, &quot;exchanges&quot;, &quot;liquidation&quot;, &quot;dropped&quot;) )) table(comp$event_group) # Active Mergers Exchanges Liquidation Dropped # 9259 11868 857 1916 9436 # [US listing gap] To obtain annual counts of the number of U.S. listed domestic firms, we use the Center for Research in Security Prices (CRSP) and Compustat databases because information on firm characteristics such as size and industry is not available from the WDI/WFE data set. We use CRSP to identify firms listed on Amex, Nasdaq, or NYSE. We include U.S. common stocks (share codes 10 and 11) and exclude investment funds and trusts (Standard Industrial Classification (SIC) codes 6722, 6726, 6798, and 6799). range(comp$hsiccd) # SIC code : 0-9999 table(comp$hshrcd) # Share Code Header # filtering data : exclude SIC code(hsiccd) = 6722, 6726, 6798, and 6799 and include Share Code(hshrcd) = 10, 11 # [re-categorize delisting, US listing gap] We follow Fama and French (2004) in categorizing CRSP delist codes 200&lt;U+2013&gt;399 as mergers and codes 400 and above as delists for cause, except for codes 570 and 573 (voluntary delists), we include them as well # For now, I include Active, Liquidation, Dropped com_sub &lt;- comp %&gt;% filter(hshrcd %in% c(10, 11) &amp; !hsiccd %in% c(6722, 6726, 6798, 6799) &amp; event_group %in% c(&quot;active&quot;, &quot;liquidation&quot;, &quot;dropped&quot;)) # mutate(event_group_new = ifelse(event_group == &#39;Active&#39;, &#39;NotActive&#39;, &#39;NotActive&#39;)) save(com_sub, file = &quot;../data/com_sub.RData&quot;) #----------------------------------------------- # wrds ratios # company identifiers : # PERMNO : PERMNO # GVKEY : Global Company Key # CUSIP : CUSIP IDENTIFIER - HISTORICAL # TICKER : EXCHANGE TICKER SYMBOL - HISTORICAL # Dates : # adate : fiscal year end # qdate : fiscal quarter end # public_date : public date (date when the information becomes public) #---------------------------------------------- load(&quot;../data/comp_ratio.RData&quot;) # comp_ratio : permno / monthly time series dim(comp_ratio) # 2752175 98 # head(comp_ratio) range(comp_ratio$public_date) # &quot;1970-01-31&quot; &quot;2022-12-31&quot; # gvkey and permno can be different. PERMNO is a unique stock (share class) level identifier assigned by CRSP to all companies listed in CRSP dataset. GVKEY (Global Company Key) is a unique number assigned to each company in the Compustat-Capital IQ database. =&gt; I will use permno. so keep one gvkey if one permno has two gvkey (e.g. permno == 10258 / gvkey = 012381, 179598) com_ratio &lt;- comp_ratio %&gt;% arrange(permno, public_date) %&gt;% group_by(permno) %&gt;% distinct(public_date, .keep_all = TRUE) load(file = &quot;../data/com_sub.RData&quot;) # com_sub no1 &lt;- unique(com_sub$permno) no2 &lt;- unique(com_ratio$permno) length(no1) # 12186 length(no2) # 20691 overlap_no &lt;- intersect(no1, no2) length(overlap_no) # 9474 com_ratio &lt;- com_ratio %&gt;% select( -ffi10, -ffi10_desc, -ffi12, -ffi12_desc, -ffi17, -ffi17_desc, -ffi30, -ffi30_desc, -ffi38, -ffi38_desc, -ffi48, -ffi48_desc, -ffi49, -ffi49_desc, -ffi5, -ffi5_desc, -gicdesc, -gsector, -gvkey, -price, -ticker, -adate, -qdate ) # combining financial ratios with company listing information # filtering financial ratios whose dates are later than end date (use only the info available up to delisting) com_ratio_monthly &lt;- com_ratio %&gt;% inner_join(com_sub, by = &quot;permno&quot;) %&gt;% arrange(permno, public_date) %&gt;% select( -dlstcd, -nwperm, -nwcomp, -cusip.y, -days, -event, #-event_group, -dlstdt, -cusip.x ) %&gt;% filter(enddat &gt; public_date) save(com_ratio_monthly, file = &quot;../data/monthly.RData&quot;) #---------------------- # load(&#39;Data/monthly.RData&#39;) # com_ratio_monthly #---------------------- if (IS_QUARTERLY_DATA) { # quarterly data (checked quarterly data has month = 3, 6, 9 , 12) com_ratio_q &lt;- com_ratio_monthly %&gt;% mutate(month = format(as.Date(public_date, format = &quot;%Y-%m-%d&quot;), &quot;%m&quot;)) %&gt;% filter(month %in% c(&quot;03&quot;, &quot;06&quot;, &quot;09&quot;, &quot;12&quot;)) %&gt;% select(-month) save(com_ratio_q, file = &quot;../data/quarterly.RData&quot;) } else { # annual data (checked annual data has dates = year-12-31) com_ratio_a &lt;- com_ratio_monthly %&gt;% mutate(month = format(as.Date(public_date, format = &quot;%Y-%m-%d&quot;), &quot;%m&quot;)) %&gt;% filter(month == &quot;12&quot;) %&gt;% select(-month) save(com_ratio_a, file = &quot;../data/annually.RData&quot;) } remove(com_info, com_ratio_monthly, com_sub, comp, comp_ratio, comp0, delist) } else { if (IS_QUARTERLY_DATA) { load(&quot;../data/quarterly.RData&quot;) } else { load(&quot;../data/annually.RData&quot;) } } # load initial data set com_ratio &lt;- NULL if (IS_QUARTERLY_DATA) { com_ratio &lt;- com_ratio_q remove(com_ratio_q) } else { com_ratio &lt;- com_ratio_a remove(com_ratio_a) } Tests of completeness and coherence of data frame com_ratio. Tests passed com_ratio &lt;- com_ratio %&gt;% ungroup() # test: public_date &lt;= LAST_TRADING_DATE_AVAILABLE expect_true(com_ratio %&gt;% select(public_date) %&gt;% summarize(max = max(public_date)) &lt;= LAST_TRADING_DATE_AVAILABLE) # test: public_date &gt;= FIRST_TRADING_DATE_AVAILABLE expect_true(com_ratio %&gt;% select(public_date) %&gt;% summarize(min = min(public_date)) &gt;= FIRST_TRADING_DATE_AVAILABLE) # test: enddat &lt;= LAST_TRADING_DATE_AVAILABLE expect_true(com_ratio %&gt;% select(enddat) %&gt;% summarize(max = max(enddat)) &lt;= LAST_TRADING_DATE_AVAILABLE) # test: enddat &gt;= FIRST_TRADING_DATE_AVAILABLE expect_true(com_ratio %&gt;% select(enddat) %&gt;% summarize(min = min(enddat)) &gt;= FIRST_TRADING_DATE_AVAILABLE) # test: every company has exactly one begdat expect_true(all(com_ratio %&gt;% group_by(permno) %&gt;% summarize(begdat_diff = as.numeric(max(begdat) - min(begdat))) %&gt;% select(begdat_diff) == 0)) # test: every company has exactly one enddat expect_true(all(com_ratio %&gt;% group_by(permno) %&gt;% summarize(begdat_diff = as.numeric(max(enddat) - min(enddat))) %&gt;% select(begdat_diff) == 0)) # test: every company is listed for a positive amount of days expect_true(all(com_ratio %&gt;% mutate(days_listed = as.numeric(enddat - begdat)) %&gt;% select(days_listed) &gt; 0)) Select columns of interest for further analyses # store information about company feature names and their identifiers com.features &lt;- tibble( name = c( &quot;Permanent Security Identification Number (unique)&quot;, &quot;Public Date&quot;, &quot;Standard Industrial Classification (SIC) Code - Header&quot;, &quot;Share Code - Header&quot;, &quot;Company Name - Header&quot;, &quot;Begin of Stock Data&quot;, &quot;End of Stock Data&quot;, &quot;Delisting Indicator&quot; ), identifier = c( &quot;permno&quot;, &quot;public_date&quot;, &quot;hsiccd&quot;, &quot;hshrcd&quot;, &quot;hcomnam&quot;, &quot;begdat&quot;, &quot;enddat&quot;, &quot;event_group&quot; ) ) # store information about financial ratio feature names and their identifiers com.features.fin &lt;- tibble( name = c( &quot;Capitalization Ratio&quot;, &quot;Asset Turnover&quot;, &quot;Inventory Turnover&quot;, &quot;Receivables Turnover&quot;, &quot;Payables Turnover&quot;, &quot;Cash Ratio&quot;, &quot;Current Ratio&quot;, &quot;Quick Ratio&quot;, &quot;After-tax Return on Average Common Equity&quot;, &quot;After-tax Return on Total Stockholders&lt;U+2019&gt; Equity&quot;, &quot;Gross Profit Margin&quot;, &quot;Operating Profit Margin After Depreciation&quot;, &quot;Operating Profit Margin Before Depreciation&quot;, &quot;Return on Assets&quot;, &quot;Return on Equity&quot;, &quot;Debt to Equity Ratio&quot;, &quot;Debt Ratio&quot;, &quot;Solvency Ratio (Liabilities / Total Assets)&quot;, &quot;Price-to-Earnings&quot; ), identifier = c( &quot;capital_ratio&quot;, &quot;at_turn&quot;, &quot;inv_turn&quot;, &quot;rect_turn&quot;, &quot;pay_turn&quot;, &quot;cash_ratio&quot;, &quot;curr_ratio&quot;, &quot;quick_ratio&quot;, &quot;aftret_eq&quot;, &quot;aftret_equity&quot;, &quot;gpm&quot;, &quot;opmad&quot;, &quot;opmbd&quot;, &quot;roa&quot;, &quot;roe&quot;, &quot;de_ratio&quot;, &quot;debt_assets&quot;, &quot;debt_at&quot;, &quot;pe_exi&quot; ) ) com &lt;- as_tibble(com_ratio) %&gt;% select( com.features$identifier, com.features.fin$identifier ) %&gt;% mutate( event_group = factor( ifelse(event_group %in% c(&quot;liquidation&quot;, &quot;dropped&quot;), &quot;delisted&quot;, &quot;active&quot;), levels = c(&quot;delisted&quot;, &quot;active&quot;) ), years_to_delisting = # convention: use the year MAX_DATE as delisting date # if &#39;2022-12-31&#39; set as enddat in initial data set ifelse(enddat &gt;= LAST_TRADING_DATE_AVAILABLE, time_length(interval(public_date, MAX_DATE), &quot;year&quot;), time_length(interval(public_date, enddat), &quot;year&quot;) ) ) %&gt;% arrange(public_date) remove(com_ratio) Macroeconomic features may improve model accuracy if delistings are affected by the overall economic situation. This behavior seems reasonable and will be checked in the exploratory data analysis. The data set with macroeconomic data contains the annual rate of change of four indices quarterly. * gdpc1: rate of change of the Real Gross Domestic Product, the inflation adjusted value of the goods and services produced by labor and property located in the United States * cpiaucsl: rate of change of the Consumer Price Index for All Urban Consumers * fredfunds: rate of change of the Federal Funds Effective Rate, the average interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight * sp500: rate of change of the S&amp;P 500 index which includes 500 leading companies in the United States and covers approximately 80% of the available market capitalization. The source of these index values is Bloomberg. This chunk loads annual changes of macroeconomic data into the data frame macro.ratios in quartetly intervals. # compute rate of change for a vector with quarterly entries rate_of_change &lt;- function(vector) { n &lt;- length(vector) return(c(0, 0, 0, 0, vector[5:n] / vector[1:(n - 4)] - 1)) } # compute absolute annual difference for a vector with quarterly entries one_year_difference &lt;- function(vector) { n &lt;- length(vector) return(c(0, 0, 0, 0, vector[5:n] - vector[1:(n - 4)])) } # gdpc1 Real Gross Domestic Product (https://fred.stlouisfed.org/series/GDPC1) # cpiaucsl Consumer Price Index for All Urban Consumers: All Items in U.S. City Average (https://fred.stlouisfed.org/series/CPIAUCSL) # fredfunds Federal Funds Effective Rate (https://fred.stlouisfed.org/series/FEDFUNDS) # sp500 annual rate of return from Bloomberg Terminal macro.measures &lt;- read_csv(&quot;../data/macroeconomic_measures.csv&quot;) macro.ratios &lt;- macro.measures %&gt;% mutate( gdpc1_chg = rate_of_change(gdpc1), cpiaucsl_chg = rate_of_change(cpiaucsl), fredfunds_chg = one_year_difference(fredfunds), sp500_chg = rate_of_change(sp500) ) %&gt;% select(-gdpc1, -cpiaucsl, -fredfunds, -sp500) %&gt;% filter(FIRST_TRADING_DATE_AVAILABLE &lt;= observation_date &amp; observation_date &lt;= LAST_TRADING_DATE_AVAILABLE) macro.ratios.name &lt;- c( gdpc1_chg = &quot;GDP (rate of change)&quot;, cpiaucsl_chg = &quot;CPI (rate of change)&quot;, fredfunds_chg = &quot;federal funds rate (1-year diff.)&quot;, sp500_chg = &quot;S&amp;P 500 (rate of change)&quot; ) pander(head(macro.ratios)) observation_date gdpc1_chg cpiaucsl_chg fredfunds_chg sp500_chg 1970-03-31 0.001628 0.06038 -0.44 -0.117 1970-06-30 0.004226 0.05686 -2.27 -0.2558 1970-09-30 -0.001667 0.056 -3.37 -0.09568 1970-12-31 0.02697 0.04811 -4.71 0.0009776 1971-03-31 0.03107 0.04315 -3.32 0.1192 1971-06-30 0.03006 0.04271 -1.23 0.371 2.2 Exploratory Data Analysis Helper functions to truncate data #&#39; winsorizes/truncates values for a given vector and quantile ranges #&#39; #&#39; @param vector vector with numerical values #&#39; @param lower_quantile_boundary lower quantile boundary #&#39; @param upper_quantile_boundary upper quantile boundary #&#39; #&#39; @return winsorized vector with min(vector) = lower_quantile and #&#39; max(vector) = upper_quantile winsorize &lt;- function(vector, lower_quantile_boundary = 0.05, upper_quantile_boundary = 0.95) { if (lower_quantile_boundary &gt; upper_quantile_boundary) { warning(&quot;Lower quantile boundary is greater than upper quantile boundary.&quot;) } lower_quantile &lt;- quantile(vector, lower_quantile_boundary, na.rm = TRUE) upper_quantile &lt;- quantile(vector, upper_quantile_boundary, na.rm = TRUE) vector[!is.na(vector) &amp; vector &lt; lower_quantile] &lt;- lower_quantile vector[!is.na(vector) &amp; vector &gt; upper_quantile] &lt;- upper_quantile return(vector) } #&#39; winsorizes/truncate columns of a given tibble #&#39; #&#39; @param data tibble with data #&#39; @param feature_identifier vector with strings of column names of tibble #&#39; which should be winsorized #&#39; @param lower_quantile_boundary lower quantile boundary #&#39; @param upper_quantile_boundary upper quantile boundary #&#39; #&#39; @return tibble of the same structure as given with winsorized columns data.prepare.winsorize &lt;- function(data, feature_identifier, lower_quantile_boundary = 0.05, upper_quantile_boundary = 0.95) { data &lt;- data %&gt;% ungroup() %&gt;% mutate(across(all_of(feature_identifier), .fns = ~ winsorize(.x, lower_quantile_boundary, upper_quantile_boundary) )) return(data) } 2.2.1 Overview over deslisting dates # histogram with number of years until delisting com %&gt;% group_by(permno) %&gt;% summarize(max_years_to_delisting = max(years_to_delisting)) %&gt;% ggplot(aes(x = max_years_to_delisting)) + geom_histogram(binwidth = 1) + xlim(c(0, ceiling(NUMBER_YEARS_AVAILABLE))) + labs( title = &quot;Number of years until delisting&quot;, x = &quot;number of years until delisting&quot;, y = &quot;number of companies&quot; ) The histogram displays the number of years until delisting for each data row of the delisted companies in the period from 1970 to 2022. This chunk creates a plot that displays the number of listed and delisted companies over time and compares them to changes in four macroeconomic ratios. # Function to scale secondary axis scale_function &lt;- function(x, scale, shift) { return(x * scale + shift) } # Function to scale secondary variable values inv_scale_function &lt;- function(x, scale, shift) { return((x + shift) / scale) } # number of delistings in subsequent year com.listed_years &lt;- com %&gt;% filter(month(public_date) == 6) %&gt;% group_by(year = floor_date(public_date, &quot;year&quot;)) %&gt;% summarize(number_of_listed_companies = n()) # number of delistings in subsequent year com.delisting_years &lt;- com %&gt;% filter(event_group == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% group_by(year = floor_date(enddat, &quot;year&quot;)) %&gt;% summarize(number_of_delistings = n()) com.delisting_years &lt;- com.delisting_years %&gt;% left_join(com.listed_years, by = join_by(year)) %&gt;% mutate(rel_delisting_frequency = number_of_delistings / number_of_listed_companies) subtitle &lt;- glue( &#39;&lt;span style = &quot;color:{colors[&quot;gray&quot;]}&quot;&gt;**Number of listed companies**&lt;/span&gt;, &#39;, &#39;&lt;span style = &quot;color:{colors[&quot;blue&quot;]}&quot;&gt;**absolute delisting frequency**&lt;/span&gt; and &lt;br&gt;&#39;, &#39;&lt;span style = &quot;color:{colors[&quot;orange&quot;]}&quot;&gt;**relative delisting frequency** (proportion of delisted companies)&lt;/span&gt;&#39; ) min_y_first &lt;- min(com.delisting_years$number_of_delistings) max_y_first &lt;- max(com.delisting_years$number_of_delistings) min_y_second &lt;- min(com.delisting_years$rel_delisting_frequency) max_y_second &lt;- max(com.delisting_years$rel_delisting_frequency) # scale and shift variables calculated based on desired mins and maxes scale &lt;- (max_y_second - min_y_second) / (max_y_first - min_y_first) shift &lt;- 0 scale1 &lt;- (max(com.delisting_years$number_of_listed_companies) - min(com.delisting_years$number_of_listed_companies)) / (max_y_first - min_y_first) shift1 &lt;- 0 plot_stock_listings_delistings &lt;- ggplot(data = com.delisting_years, mapping = aes(x = year)) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;1985-01-01&quot;), xmax = ymd(&quot;1987-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;2000-01-01&quot;), xmax = ymd(&quot;2002-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;2008-01-01&quot;), xmax = ymd(&quot;2010-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;1990-01-01&quot;), xmax = ymd(&quot;1992-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + geom_line( mapping = aes(y = inv_scale_function(number_of_listed_companies, scale1, shift1), color = &quot;number of listings&quot;), linetype = &quot;dotted&quot;, linewidth = 1 ) + annotate( geom = &quot;text&quot;, x = max(com.delisting_years$year), y = inv_scale_function(max(com.delisting_years$number_of_listed_companies), scale1, shift1), label = max(com.delisting_years$number_of_listed_companies), hjust = 1.2 ) + annotate( geom = &quot;text&quot;, x = min(com.delisting_years$year), y = inv_scale_function(min(com.delisting_years$number_of_listed_companies), scale1, shift1), label = min(com.delisting_years$number_of_listed_companies), hjust = 1.2 ) + geom_line( mapping = aes(y = inv_scale_function(rel_delisting_frequency, scale, shift), color = &quot;relative delisting frequncy&quot;), linetype = &quot;longdash&quot;, linewidth = 1 ) + geom_line( mapping = aes(y = number_of_delistings, color = &quot;Absolute delisting frequency&quot;), linetype = &quot;solid&quot;, linewidth = 1 ) + scale_color_manual(values = c( &quot;number of listings&quot; = as.character(colors[&quot;gray&quot;]), &quot;Absolute delisting frequency&quot; = as.character(colors[&quot;blue&quot;]), &quot;relative delisting frequncy&quot; = as.character(colors[&quot;orange&quot;]) )) + scale_y_continuous( &quot;Abslolute delisting frequency&quot;, sec.axis = sec_axis(~ scale_function(., scale, shift), name = &quot;Relative delisting frequency&quot;, labels = waiver()) ) + labs( title = &quot;Delistings vs. calendar year&quot;, subtitle = subtitle, x = &quot;calendar year&quot;, ) + theme( plot.subtitle = element_markdown(), axis.title.y = element_markdown(color = colors[&quot;blue&quot;]), axis.title.y.right = element_markdown(color = colors[&quot;orange&quot;]), legend.position = &quot;none&quot; ) plot_macro_ratios &lt;- ggplot( data = macro.ratios %&gt;% pivot_longer( cols = -observation_date, names_to = &quot;index_chg&quot;, values_to = &quot;value&quot; ), aes(x = observation_date, y = value) ) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;1985-01-01&quot;), xmax = ymd(&quot;1987-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;2000-01-01&quot;), xmax = ymd(&quot;2002-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;2008-01-01&quot;), xmax = ymd(&quot;2010-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + annotate(geom = &quot;rect&quot;, xmin = ymd(&quot;1990-01-01&quot;), xmax = ymd(&quot;1992-01-01&quot;), ymin = -Inf, ymax = Inf, fill = colors[&quot;green&quot;], alpha = 0.2) + geom_line(linewidth = 1) + facet_wrap(. ~ index_chg, scales = &quot;free_y&quot;, labeller = labeller(index_chg = macro.ratios.name)) + labs( title = &quot;Macro economic index changes&quot;, x = &quot;calendar year&quot;, y = &quot;one-year change of index&quot; ) ggarrange(plot_stock_listings_delistings, plot_macro_ratios, ncol = 2, nrow = 1) # months of delisting com %&gt;% # filter only last quarter before delisting filter(years_to_delisting &lt;= 0.25 &amp; event_group != &quot;active&quot;) %&gt;% group_by(permno, enddat) %&gt;% mutate(delisting_month = lubridate::month(enddat, label = TRUE)) %&gt;% ggplot(aes(x = delisting_month)) + geom_bar() + labs( title = &quot;Number of delistings&quot;, x = &quot;month&quot;, y = &quot;Frequency&quot; ) Systematic seasonality of delistings is not observable. 2.2.2 Correlation analysis correlation.info &lt;- function(data, feature_identifier) { correlation.matrix &lt;- data %&gt;% select(all_of(feature_identifier)) %&gt;% cor(x = , use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;) %&gt;% as.data.frame() %&gt;% rownames_to_column(var = &quot;rowname&quot;) correlation.plot &lt;- correlation.matrix %&gt;% pivot_longer( cols = -rowname, names_to = &quot;rowname1&quot;, values_to = &quot;correlation&quot; ) %&gt;% ggplot(aes(rowname, rowname1, fill = correlation)) + geom_tile() + scale_fill_gradient2( low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0, space = &quot;Lab&quot;, name = &quot;Pearson\\nCorrelation&quot; ) + labs( title = &quot;Correlation Matrix&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme( aspect.ratio = 1, axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1) ) return(list(matrix = correlation.matrix, plot = correlation.plot)) } correlation.info.initial &lt;- correlation.info(com, com.features.fin$identifier) correlation.info.initial$plot The majority of features is not correlated with a Pearson‚Äôs correlation coefficient of around 0. The operative profit margin ratios opmad and opmbd as well as the liquidity ratios curr_ratio and quick_ratio are perfectly positively correlated. The correlation coefficient between the debt ratios debt_assets and debt_at is 0.76. The remaining correlation coefficients are all lower than 0.75. Therefore, the next chunk searches all pairwise correlations and removes one of the pairs if its absolute value is higher than the CORRELATION_THRESHOLD. # winsorize features com.winsorized &lt;- data.prepare.winsorize(com, com.features.fin$identifier, lower_quantile_boundary = 0.05, upper_quantile_boundary = 0.95 ) n_features &lt;- length(com.features.fin$identifier) feature_indicators_to_remove &lt;- character(0) # check features mutually if they meet CORRELATION_THRESHOLD and remove one of them for (i in 1:(n_features - 1)) { feature_identifier_i &lt;- com.features.fin$identifier[i] for (j in (i + 1):n_features) { feature_identifier_j &lt;- com.features.fin$identifier[j] if (abs(correlation.info.initial$matrix[i, j + 1]) &gt;= CORRELATION_THRESHOLD) { # scatter plot of correlated features &gt; truncation tbd print(ggplot(com.winsorized, aes_string(x = feature_identifier_i, y = feature_identifier_j)) + geom_point() + geom_smooth() + labs(title = paste(&quot;Scatterplot of&quot;, feature_identifier_i, &quot;vs&quot;, feature_identifier_j))) print(paste0( &quot;Correlation between features &quot;, feature_identifier_i, &quot; and &quot;, feature_identifier_j, &quot; is &quot;, formatC(correlation.info.initial$matrix[i, j + 1], digits = 4), &quot;. Therefore, feature &quot;, feature_identifier_j, &quot; will be removed in further analyses.&quot; )) feature_indicators_to_remove &lt;- c(feature_indicators_to_remove, feature_identifier_j) } } } ## [1] &quot;Correlation between features curr_ratio and quick_ratio is 0.9965. Therefore, feature quick_ratio will be removed in further analyses.&quot; ## [1] &quot;Correlation between features opmad and opmbd is 1. Therefore, feature opmbd will be removed in further analyses.&quot; ## [1] &quot;Correlation between features debt_assets and debt_at is 0.7643. Therefore, feature debt_at will be removed in further analyses.&quot; The two scatterplots of the highly correlated (winsorized) features opmad and opmbd as well as curr_ratio and quick_ratio confirm linear relationship between them. The scatterplot between debt_assets and debt_at shows non-linear structures for higher values. 2.2.3 Summary statistics for features com.summary &lt;- com %&gt;% pivot_longer( cols = com.features.fin$identifier, names_to = &quot;feature&quot;, values_to = &quot;value&quot; ) %&gt;% group_by(feature) %&gt;% summarize( min = min(value, na.rm = TRUE), quantile1 = quantile(value, 0.01, na.rm = TRUE), quantile25 = quantile(value, 0.25, na.rm = TRUE), median = median(value, na.rm = TRUE), mean = mean(value, na.rm = TRUE), quantile75 = quantile(value, 0.75, na.rm = TRUE), quantile99 = quantile(value, 0.99, na.rm = TRUE), max = max(value, na.rm = TRUE), .groups = &quot;drop&quot; ) pander(com.summary) Table continues below feature min quantile1 quantile25 median mean aftret_eq -19953 -5.571 -0.1625 0.06951 -0.1111 aftret_equity -3462 -5.028 -0.1707 0.06657 -0.03336 at_turn 1.864e-17 0.007323 0.3119 0.8545 1.052 capital_ratio -1730 0 0.01598 0.2046 0.2998 cash_ratio -0.1999 0.002325 0.1199 0.4543 2.087 curr_ratio -0.002398 0.2088 1.314 2.048 3.513 de_ratio -43363 -12.28 0.4065 1.052 2.455 debt_assets 0 0.03813 0.3243 0.5387 0.5883 debt_at -0.000114 0 0.04189 0.1809 0.2523 gpm -29326 -30 0.1836 0.3323 -4.023 inv_turn -49.38 0.09632 2.612 4.87 49.23 opmad -1.91e+16 -54.76 -0.05254 0.05746 -5.159e+10 opmbd -1.845e+16 -52.46 -0.005601 0.0922 -4.984e+10 pay_turn -49450 -0.03247 3.834 8.089 22.27 pe_exi -2375 -446.3 -4.186 8.391 0.1431 quick_ratio -0.02353 0.1265 0.8619 1.409 2.914 rect_turn 9.09e-17 0.05799 3.777 6.044 21.52 roa -1315 -1.373 -0.01829 0.07031 -0.02089 roe -15951 -4.396 -0.153 0.0631 3.573 quantile75 quantile99 max 0.1617 3.877 21775 0.1576 3.06 21775 1.496 4.368 543 0.4508 1.518 6722 1.542 23.19 9084 3.406 25.09 1584 2.532 25.24 56626 0.7523 1.538 937.5 0.3639 1.022 496.5 0.5261 0.9464 2.628 12.41 420.9 581233 0.1455 0.5756 37.45 0.1991 0.6589 37.45 13.68 87.48 878016 19.31 299.6 722.5 2.614 24.36 1584 9.207 205.8 702131 0.1539 0.4779 102 0.1482 1.111 377126 # qq-plots for features com %&gt;% pivot_longer( cols = com.features.fin$identifier, names_to = &quot;feature&quot;, values_to = &quot;value&quot; ) %&gt;% ggplot(aes(y = value)) + geom_boxplot() + facet_wrap(~feature, scales = &quot;free&quot;) The summary table and boxplots show that the features \\(\\texttt{inv_turn}\\), \\(\\texttt{opmad}\\), \\(\\texttt{opmbd}\\) and \\(\\texttt{rect_turn}\\) and \\(\\texttt{roe}\\) contain extremely large values in absolute values. 2.2.4 Features and delisting frequency The following histograms show for each feature in the data set the number of data rows that fall in a specific range in gray (x-axis). The blue bars represent the number of financial ratio rows in a specific range that belong to a company which delisted in the subsequent year (gap of three month included). For example, if a company was delisted in April 2002, then the financial ratios from January 2001 to January 2002 would be attributed to the blue bar. The ratio between the blue and gray bar, named relative delisting frequency, is displayed by the orange points and a corresponding smoothed line (method LOESS with tri-cubic kernel). The right y-axis is the scale for the relative frequency values. The feature values are winsorized at the 5% and 95% quantile. #&#39; creates a histogram of features vs delisting frequency #&#39; #&#39; @param data tibble containing financial ratios for companies with a column #&#39; named feature_identifier and years_to_delisting #&#39; @param feature_identifier technical identifier of feature of interest #&#39; @param feature_description readable name of feature for title #&#39; @param marginal.data vector of predicted delistings #&#39; @param marginal.legend legend for marginal plots #&#39; #&#39; @return histogram of fincancial ratio features with up to two smoothed #&#39; relative delisting frequencies hist_delisting_frequency &lt;- function( data, feature_identifier, feature_description, marginal.data = NULL, marginal.legend = &quot;Modeled delisting frequency&quot;, marginal.data2 = NULL, marginal.legend2 = &quot;&quot;) { feature &lt;- data[[feature_identifier]] # first column years_to_delisting &lt;- data$years_to_delisting # first column max_x &lt;- max(feature, na.rm = TRUE) min_x &lt;- min(feature, na.rm = TRUE) n_bins &lt;- 30 bin_boundaries &lt;- seq(min_x, max_x, length.out = n_bins + 1) bin_width &lt;- bin_boundaries[2] - bin_boundaries[1] plot.data &lt;- tibble( feature = feature, years_to_delisting = years_to_delisting, feature_bin_index = findInterval(feature, bin_boundaries), bin_centers = bin_boundaries[1] + bin_width * (feature_bin_index - 1.5) ) # compute points if (!is.null(marginal.data)) { marginal.data &lt;- plot.data %&gt;% mutate(pred = marginal.data) %&gt;% group_by(bin_centers) %&gt;% summarize(relative_frequency_delisting_pred = mean(pred)) %&gt;% select(x = bin_centers, yhat = relative_frequency_delisting_pred) } # compute points if (!is.null(marginal.data2)) { marginal.data2 &lt;- plot.data %&gt;% mutate(pred = marginal.data2) %&gt;% group_by(bin_centers) %&gt;% summarize(relative_frequency_delisting_pred = mean(pred)) %&gt;% select(x = bin_centers, yhat = relative_frequency_delisting_pred) } # max of first y axis max_y_first &lt;- plot.data %&gt;% group_by(feature_bin_index) %&gt;% summarize(frequency = n()) %&gt;% select(frequency) %&gt;% max() max_y_second &lt;- 0.3 # max of second y axis min_y_first &lt;- 0 # min of first y axis min_y_second &lt;- 0 # min of second y axis # scale and shift variables calculated based on desired mins and maxes scale &lt;- (max_y_second - min_y_second) / (max_y_first - min_y_first) shift &lt;- min_y_first - min_y_second com.frequency_delisting &lt;- plot.data %&gt;% mutate(delisting_indicator = determine_delisting_property(years_to_delisting)) %&gt;% group_by(bin_centers) %&gt;% summarize( frequency = n(), frequency_delisting = sum(delisting_indicator) ) %&gt;% mutate(relative_frequency_delisting = frequency_delisting / frequency) plot &lt;- ggplot(plot.data) + geom_histogram( aes(x = feature, color = &quot;Absolute frequency&quot;, fill = &quot;Absolute frequency&quot;), binwidth = bin_width ) + geom_histogram( aes( x = feature * ifelse(determine_delisting_property(years_to_delisting), 1, NA), color = &quot;Absolute delisting frequency&quot;, fill = &quot;Absolute delisting frequency&quot; ), binwidth = bin_width ) + geom_point( data = com.frequency_delisting, mapping = aes( x = bin_centers, y = inv_scale_function(relative_frequency_delisting, scale, shift), color = &quot;Relative delisting frequency&quot; ) ) if (feature_identifier == &quot;pe_exi&quot;) { plot &lt;- plot + geom_smooth( data = com.frequency_delisting %&gt;% filter(bin_centers &lt;= 0), method = &quot;loess&quot;, mapping = aes( x = bin_centers, y = inv_scale_function(relative_frequency_delisting, scale, shift), color = &quot;Relative delisting frequency&quot; ) ) + geom_smooth( data = com.frequency_delisting %&gt;% filter(bin_centers &gt; 0), mapping = aes( x = bin_centers, y = inv_scale_function(relative_frequency_delisting, scale, shift), color = &quot;Relative delisting frequency&quot; ) ) if (!is.null(marginal.data)) { plot &lt;- plot + geom_point( data = marginal.data %&gt;% filter(x &lt;= 0), mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ) ) + geom_smooth( data = marginal.data %&gt;% filter(x &lt;= 0), se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ), linetype = &quot;dashed&quot; ) + geom_point( data = marginal.data %&gt;% filter(x &gt; 0), mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ) ) + geom_smooth( data = marginal.data %&gt;% filter(x &gt; 0), se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ), linetype = &quot;dashed&quot; ) } if (!is.null(marginal.data2)) { plot &lt;- plot + geom_point( data = marginal.data2 %&gt;% filter(x &lt;= 0), mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ) ) + geom_smooth( data = marginal.data2 %&gt;% filter(x &lt;= 0), se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ), linetype = &quot;dotted&quot; ) + geom_point( data = marginal.data2 %&gt;% filter(x &gt; 0), mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ) ) + geom_smooth( data = marginal.data2 %&gt;% filter(x &gt; 0), se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ), linetype = &quot;dotted&quot; ) } } else { plot &lt;- plot + geom_smooth( data = com.frequency_delisting, mapping = aes( x = bin_centers, y = inv_scale_function(relative_frequency_delisting, scale, shift), color = &quot;Relative delisting frequency&quot; ) ) if (!is.null(marginal.data)) { plot &lt;- plot + geom_point( data = marginal.data, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ) ) + geom_smooth( data = marginal.data, se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend ), linetype = &quot;dashed&quot; ) } if (!is.null(marginal.data2)) { plot &lt;- plot + geom_point( data = marginal.data2, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ) ) + geom_smooth( data = marginal.data2, se = FALSE, mapping = aes( x = x, y = inv_scale_function(yhat, scale, shift), color = marginal.legend2 ), linetype = &quot;dotted&quot; ) } } subtitle &lt;- glue( &#39;&lt;span style=&quot;color:{colors[&quot;gray&quot;]}&quot;&gt;**Absolute frequency (active and delisted)**&lt;/span&gt;, &#39;, &#39;&lt;span style=&quot;color:{colors[&quot;blue&quot;]}&quot;&gt;**Absolute delisting frequency**&lt;/span&gt;, &lt;br&gt;&#39;, &#39;&lt;span style=&quot;color:{colors[&quot;orange&quot;]}&quot;&gt;**Relative delisting frequency**&lt;/span&gt;&#39;, ifelse(marginal.legend == &quot;&quot;, &quot;&quot;, paste0(&#39;, &lt;span style=&quot;color:{colors[&quot;magenta&quot;]}&quot;&gt;**&#39;, marginal.legend, &quot;**&lt;/span&gt;&quot;)), ifelse(marginal.legend2 == &quot;&quot;, &quot;&quot;, paste0(&#39;,&lt;br&gt;&lt;span style=&quot;color:{colors[&quot;lightblue&quot;]}&quot;&gt;**&#39;, marginal.legend2, &quot;**&lt;/span&gt;&quot;)) ) plot &lt;- plot + scale_y_continuous( name = &quot;Frequency&quot;, limits = c(min_y_first, max_y_first), sec.axis = sec_axis(~ scale_function(., scale, shift), name = &quot;Relative delisting frequency&quot; ), oob = scales::rescale_none ) + scale_color_manual( breaks = c( &quot;Absolute frequency&quot;, &quot;Absolute delisting frequency&quot;, &quot;Relative delisting frequency&quot;, marginal.legend, marginal.legend2 ), values = c( as.character(colors[&quot;gray&quot;]), as.character(colors[&quot;blue&quot;]), as.character(colors[&quot;orange&quot;]), as.character(colors[&quot;magenta&quot;]), as.character(colors[&quot;lightblue&quot;]) ) ) + scale_fill_manual( values = c( &quot;Absolute frequency&quot; = as.character(colors[&quot;gray&quot;]), &quot;Absolute delisting frequency&quot; = as.character(colors[&quot;blue&quot;]), &quot;Relative delisting frequency&quot; = as.character(colors[&quot;orange&quot;]), as.character(colors[&quot;magenta&quot;]), as.character(colors[&quot;lightblue&quot;]) ) ) + labs( title = paste0(&quot;Histogram of &quot;, feature_description), subtitle = subtitle, x = feature_identifier ) + theme( plot.subtitle = element_markdown(), axis.title.y = element_markdown(color = colors[&quot;gray&quot;]), axis.title.y.right = element_markdown(color = colors[&quot;orange&quot;]), legend.position = &quot;none&quot; ) + geom_text( aes( x = max_x, y = max_y_first, label = &quot;outliers are winsorized&quot; ), stat = &quot;unique&quot;, hjust = 1, size = 3 ) return(plot) } # winsorize features com.winsorized &lt;- data.prepare.winsorize(com, com.features.fin$identifier, lower_quantile_boundary = 0.05, upper_quantile_boundary = 0.95 ) # plot histogram of features and delisting frequency plots &lt;- list() for (i in 1:length(com.features.fin$identifier)) { feature_description &lt;- com.features.fin[[i, 1]] feature_identifier &lt;- com.features.fin[[i, 2]] plots[[feature_identifier]] &lt;- hist_delisting_frequency( com.winsorized %&gt;% select(all_of(feature_identifier), years_to_delisting), feature_identifier, feature_description ) print(plots[[feature_identifier]]) } Capitalization and Solvency ratios - capital_ratio, de_ratio, debt_assets, debt_at: The higher the ratio of debt to assets or liabilities, the higher the relative delisting frequency in the subsequent year. Efficiency - at_turn, inv_turn, rect_turn, pay_turn: These efficiency ratios but pay_turn seem to have no observable marginal effect on delisting. However, if the ratio between cost of goods sold plus the change in inventories and accounts payable gets larger, the observed delisting frequency for the subsequent year decreases. Liquidity - cash_ratio, curr_ratio, quick_ratio: The ratio between cash plus short-term investments (cash_ratio) seems to have no marginal predictive power for delisting. However, if the ratioto current liabilities (curr_ratio, quick_ratio) increases, the relative frequency of delistings in the subsequent year decreases Profitability - aftret_eq, aftret_equity, gpm, opmad, opmbd, roa, roe: If these profitability ratios are negative, the observed relative frequency for delisting in the subsequent year is relatively high compared to the cases in which the ratios are positive. The denominators of these profitability ratios contain assets or sales key figures which are greater than or equal to zero. So if the income or sales key figures in the numerator of the ratios are negative, the company‚Äôs situation is on average more problematic with respect to delisting. Valuation - pe_exi: For positive price-to-earnings ratios (per share) the observed delisting frequency in the subsequent year becomes smaller for increasing ratios. The higher the price-to-earnings ratio, the better is a share valued. If the price-to-earnings ratio is negative, then the earnings in the denominator of the ratio are negative. If the loss per share is relatively small, then the price-to-earnings ratio is ‚â™ ‚àí1. Hence, decreasing ratios are associated with a lower relative delisting frequency. (plots[[&quot;capital_ratio&quot;]] | plots[[&quot;curr_ratio&quot;]]) / (plots[[&quot;roe&quot;]] | plots[[&quot;pe_exi&quot;]]) 2.3 Predictor data set 2.3.1 Identifiy duplicates # number of duplicate rows paste(&quot;Number of duplicate rows:&quot;, com %&gt;% duplicated() %&gt;% sum()) ## [1] &quot;Number of duplicate rows: 0&quot; 2.3.2 Missing data #&#39; Missing data profile as bar chart #&#39; #&#39; @param data tibble containing financial ratios for companies #&#39; #&#39; @return bar chart showing percentages of missing data for each feature in data plot_missing_data_profile &lt;- function(data) { data_missing_data_profile &lt;- data %&gt;% summarize(across(everything(), ~ sum(is.na(.x)))) %&gt;% pivot_longer( cols = everything(), names_to = &quot;variable&quot;, values_to = &quot;number_missing_values&quot; ) %&gt;% mutate(percent_missing_values = number_missing_values / nrow(com)) plot_missing_data_profile &lt;- data_missing_data_profile %&gt;% ggplot( aes(x = reorder(variable, -number_missing_values), y = percent_missing_values) ) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + geom_text(aes(label = paste0(format(100 * percent_missing_values, digits = 2), &quot;%&quot;)), hjust = 1, colour = &quot;white&quot; ) + labs( title = &quot;Missing data profile&quot;, y = &quot;Percentage of missing rows&quot;, x = &quot;Features&quot; ) return(plot_missing_data_profile) } plot_missing_data_profile(com %&gt;% select(com.features.fin$identifier)) More than every fourth value for the feature \\(\\textt{inv_turn}\\) (inventory turnover) is missing. 2.3.3 Predictior data set #&#39; imputes missing values for a specific data set given #&#39; #&#39; @param data company data including financial ratio columns #&#39; @param test_indicator indicator vector with the same length as nrow(data) containing #&#39; TRUE if corresponding row in data belongs to the test set #&#39; otherwise FALSE #&#39; @param method method to use for imputation, currently &quot;pm&quot; predictive mean matching #&#39; #&#39; @return imputed data set data.prepare.impute &lt;- function(data, test_indicator, method = &quot;pmm&quot;) { data_imputed &lt;- NULL if (method == &quot;pmm&quot;) { if (REDO_MICE_IMPUTATION_PPM) { data_imputed &lt;- complete(mice(data, method = &quot;pmm&quot;, ignore = test_indicator, m = 5, visitSequence = &quot;monotone&quot;, maxit = 5 )) save(data_imputed, file = &quot;../data/com_imputed.RData&quot;) } else { load(&quot;../data/com_imputed.RData&quot;) } } return(data_imputed) } #&#39; computes historical change features for several features listed in #&#39; feature_identifier #&#39; performs regression of financial ratio values on the available report dates #&#39; of the past five years for each company and takes the fitted slope #&#39; coefficient as historical change feature #&#39; #&#39; @param data company data including financial ratio columns and column #&#39; public_date_selected #&#39; @param feature_identifier list of technical feature identifier of interest #&#39; #&#39; @return data set with historical change features data.prepare.history &lt;- function(data, feature_identifier) { # add features containing slope of historical financial data for (feature in feature_identifier) { feature_history_name &lt;- paste0(feature, &quot;_h&quot;) com_slope &lt;- data %&gt;% group_by(permno) %&gt;% # use at most 5 years of history of financial ratios filter(public_date_selected - years(5) &lt;= public_date &amp; public_date &lt;= public_date_selected) %&gt;% # compute slope coefficient on filtered and grouped subset for a 5-year time period summarize(!!feature_history_name := lm((!!as.symbol(feature)) ~ public_date)$coefficients[2]) data &lt;- data %&gt;% left_join(com_slope, by = join_by(permno)) } return(data %&gt;% replace(is.na(.), 0)) # !!!!!!!!!!!!!!!!!!!! } #&#39; prepares company data for model training and testing purposes #&#39; #&#39; @param com_data company data including financial ratio columns #&#39; @param response company data including columns permno and response which #&#39; determines response variable for each company in com_data #&#39; @param com_subset company data with permno and public_date of a (sub)set of #&#39; companies #&#39; @param macro_data macro-economic data #&#39; @param feature_identifier vector containing column names of financial ratio #&#39; features to select column in com_data #&#39; #&#39; @return data set with four subsequent report dates for each company and #&#39; public_date specified in com_subset joined with macro-economic features data.prepare &lt;- function(com_data, com.subset, macro.data, feature.identifier) { # number of companies in com_data print(com_data$permno %&gt;% unique() %&gt;% length()) data.prepared &lt;- com_data %&gt;% arrange(desc(public_date)) # join financial ratios with selected permno and public_date rows data.prepared &lt;- com_data %&gt;% right_join(com.subset, by = join_by(permno), suffix = c(&quot;&quot;, &quot;_selected&quot;)) # add historic development features (5 year history) data.prepared &lt;- data.prepare.history(data.prepared, feature.identifier) %&gt;% filter(public_date == public_date_selected) %&gt;% select(-public_date_selected) # add features with macro data ratios (gdp, inflation, interest, sp500) data.prepared &lt;- data.prepared %&gt;% left_join(macro.data, by = join_by(closest(x$public_date &gt;= y$observation_date)) ) %&gt;% select(-observation_date) # check if n_companies reduced -&gt; should not change # number of companies in com_data print(data.prepared$permno %&gt;% unique() %&gt;% length()) # check for one row per company print(paste( &quot;Does every company have exactly one row in data.prepared? -&quot;, nrow(data.prepared) == (data.prepared$permno %&gt;% unique() %&gt;% length()) )) return(data.prepared) } #&#39; sample random report date (public_date) for each company #&#39; #&#39; @param data company data including columns permno, public_date #&#39; @return tibble with columns permno and public_date containing companies and #&#39; public_date for train set com.time_points &lt;- function(data) { # contains exactly one random report date for every company, for every # delisted company a report date which is more than 3 months before delisting com.time_points.result &lt;- data %&gt;% arrange(public_date) %&gt;% group_by(permno, event_group) %&gt;% # require at least 3 subsequent quarters of financial ratios filter((event_group == &quot;delisted&quot; &amp; row_number() &gt;= 3) | (event_group == &quot;active&quot; &amp; row_number() &gt;= 3)) %&gt;% slice_sample(n = 1) %&gt;% ungroup() %&gt;% select(permno, public_date, response) return(com.time_points.result) } 2.3.4 Prepare predictor data set # remove features that meet threshold in chunk correlated_features_detect_removals com.features.fin &lt;- com.features.fin %&gt;% filter(!identifier %in% feature_indicators_to_remove) # com &lt;- com %&gt;% select(-feature_indicators_to_remove) # list companies and the number of quarters in which they reported financial ratio dates com.quarters_per_company &lt;- com %&gt;% group_by(permno) %&gt;% summarize( n_quarters = n(), .groups = &quot;drop&quot; ) paste(&quot;Number of companies in data set:&quot;, com$permno %&gt;% unique() %&gt;% length()) ## [1] &quot;Number of companies in data set: 9424&quot; paste( &quot;Number of companies &lt; 4 financial ratio dates:&quot;, nrow(com.quarters_per_company %&gt;% filter(n_quarters &lt; N_QUARTERS_FIN_RATIOS)) ) ## [1] &quot;Number of companies &lt; 4 financial ratio dates: 409&quot; paste( &quot;Number of companies &gt;= 4 financial ratio dates:&quot;, nrow(com.quarters_per_company %&gt;% filter(n_quarters &gt;= N_QUARTERS_FIN_RATIOS)) ) ## [1] &quot;Number of companies &gt;= 4 financial ratio dates: 9015&quot; # remove companies that have less than N_QUARTERS_FIN_RATIO_REQUIRED reported financial ratio dates com &lt;- com.quarters_per_company %&gt;% inner_join(com, by = join_by(permno)) %&gt;% filter((n_quarters &gt;= N_QUARTERS_FIN_RATIOS &amp; event_group == &quot;active&quot;) | (n_quarters &gt;= N_QUARTERS_FIN_RATIOS_REQUIRED &amp; event_group == &quot;delisted&quot;)) %&gt;% # exclude gap of 3 months from predictor data set filter(time_length(interval(public_date, enddat), &quot;year&quot;) &gt;= 0.25) %&gt;% select(-n_quarters) # winsorize features com &lt;- data.prepare.winsorize(com, com.features.fin$identifier) # determine response variable associated with company: &quot;delisted&quot; or &quot;active&quot; com.response &lt;- com %&gt;% mutate(response = ifelse( determine_delisting_property(time_length(interval(public_date, enddat), &quot;year&quot;)) &amp; event_group != &quot;active&quot;, &quot;delisted&quot;, &quot;active&quot; ) %&gt;% as.factor()) %&gt;% select(permno, public_date, event_group, response) set.seed(1) # sample one random public_date for each company # tibble containing columns permno and public_date for each company in com com.time_points.sample &lt;- com.time_points(com.response) # train test split (75%|25%), stratified sampling based on response feature com.split &lt;- initial_split(com.time_points.sample, strata = response, prop = 0.75) com.train &lt;- training(com.split) com.test &lt;- testing(com.split) # indicate for each company if it belongs to training or testing set com &lt;- com %&gt;% mutate(test_indicator = !permno %in% com.train$permno) test_indicator &lt;- com$test_indicator # impute data com.imputed &lt;- data.prepare.impute( com %&gt;% select(com.features.fin$identifier), com$test_indicator, &quot;pmm&quot; ) %&gt;% mutate( permno = com$permno, public_date = com$public_date ) if (REBUILD_PREDICTOR_DATA_SET) { # training set data.training &lt;- data.prepare( com.imputed %&gt;% select(permno, public_date, com.features.fin$identifier) %&gt;% filter(!test_indicator), com.train, macro.ratios, com.features.fin$identifier ) %&gt;% mutate(response = as.factor(response)) save(data.training, file = &quot;../data/data.training.RData&quot;) # test set data.test &lt;- data.prepare( com.imputed %&gt;% select(permno, public_date, com.features.fin$identifier) %&gt;% filter(test_indicator), com.test, macro.ratios, com.features.fin$identifier ) %&gt;% mutate(response = as.factor(response)) save(data.test, file = &quot;../data/data.test.RData&quot;) } else { load(&quot;../data/data.training.RData&quot;) load(&quot;../data/data.test.RData&quot;) } Test historical features of predictor data set for coherence by choosing random sample row Tests passed data.sample &lt;- data.training[sample(1:nrow(data.training), size = 1), ] data.sample.history &lt;- com %&gt;% filter( permno == data.sample$permno, data.sample$public_date - years(5) &lt;= public_date &amp; public_date &lt;= data.sample$public_date ) %&gt;% select(public_date, capital_ratio) data.sample.history.lm &lt;- lm(capital_ratio ~ public_date, data = data.sample.history) # ggplot(data = data.sample.history, aes(public_date, capital_ratio)) + # geom_point() + # geom_abline(slope = data.sample.history.lm$coefficients[2], # intercept = data.sample.history.lm$coefficients[1]) expect_true(data.sample$capital_ratio_h == data.sample.history.lm$coefficients[2]) paste( &quot;Total number of companies:&quot;, com.quarters_per_company %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Total number of companies: 9424&quot; paste( &quot;Number of delisted companies:&quot;, com %&gt;% filter(event_group == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Number of delisted companies: 5417&quot; paste( &quot;Total number of companies w/ at least five quarters:&quot;, com %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Total number of companies w/ at least five quarters: 8870&quot; paste( &quot;Proportion of delisted companies:&quot;, formatC(com %&gt;% filter(!event_group %in% &quot;active&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() / com %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow(), digits = 4) ) ## [1] &quot;Proportion of delisted companies: 0.6107&quot; paste( &quot;Median number of years of financial ratios for active companies:&quot;, (com %&gt;% filter(event_group == &quot;active&quot;) %&gt;% mutate(n_active_years = time_length(interval(begdat, LAST_TRADING_DATE_AVAILABLE), &quot;year&quot;)))$n_active_years %&gt;% median(., na.rm = TRUE) %&gt;% formatC(., digits = 4) ) ## [1] &quot;Median number of years of financial ratios for active companies: 28.89&quot; paste(&quot;Proportion of companies in train set:&quot;, formatC(nrow(data.training) / (nrow(data.training) + nrow(data.test)), digits = 2)) ## [1] &quot;Proportion of companies in train set: 0.75&quot; paste( &quot;Number of companies in train set:&quot;, data.training %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Number of companies in train set: 6652&quot; paste( &quot;Number of delisted companies in train set:&quot;, data.training %&gt;% filter(response == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Number of delisted companies in train set: 1409&quot; paste( &quot;Number of active companies in train set:&quot;, data.training %&gt;% filter(response == &quot;active&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() ) ## [1] &quot;Number of active companies in train set: 5243&quot; paste( &quot;Proportion of delisted responses in train set:&quot;, formatC(data.training %&gt;% filter(response == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() / data.training %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow(), digits = 4) ) ## [1] &quot;Proportion of delisted responses in train set: 0.2118&quot; paste( &quot;Proportion of delisted responses in test set:&quot;, formatC(data.test %&gt;% filter(response == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow() / data.test %&gt;% group_by(permno) %&gt;% count() %&gt;% nrow(), digits = 4) ) ## [1] &quot;Proportion of delisted responses in test set: 0.1966&quot; paste( &quot;Median number of years of financial ratios for active companies in train set:&quot;, formatC((data.training %&gt;% left_join(com, by = join_by(permno, public_date)) %&gt;% filter(response == &quot;active&quot;) %&gt;% mutate(n_active_years = time_length(interval(begdat, public_date), &quot;year&quot;)))$n_active_years %&gt;% median(., na.rm = TRUE), digits = 4) ) ## [1] &quot;Median number of years of financial ratios for active companies in train set: 3.923&quot; paste(&quot;Are there missing cells in train set? -&quot;, any(colSums(is.na(data.training)) &gt; 0)) ## [1] &quot;Are there missing cells in train set? - FALSE&quot; # sampled time points of financial ratios of active companies com.active.years &lt;- com.time_points.sample %&gt;% left_join(com, by = join_by(permno, public_date)) %&gt;% filter(event_group == &quot;active&quot;) %&gt;% group_by(year = floor_date(public_date, &quot;year&quot;)) %&gt;% summarize(number_of_companies = n()) com.active.frequency &lt;- com %&gt;% filter(event_group == &quot;delisted&quot;) %&gt;% group_by(permno) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% mutate(year = floor_date(ymd(enddat), &quot;year&quot;)) %&gt;% group_by(year) %&gt;% summarize( number_of_delistings = n(), .groups = &quot;drop&quot; ) %&gt;% mutate( proportion_wanted = number_of_delistings / sum(number_of_delistings) ) com.active.begdat &lt;- com.time_points.sample %&gt;% left_join(com, by = join_by(permno, public_date)) %&gt;% filter(event_group == &quot;active&quot;) %&gt;% group_by(permno) %&gt;% filter(row_number() == 1) %&gt;% ungroup() %&gt;% group_by(year = floor_date(begdat, &quot;year&quot;)) %&gt;% summarize(number_of_companies = n()) subtitle &lt;- glue( &#39;&lt;span style=&quot;color:{colors[&quot;blue&quot;]}&quot;&gt;**Number of companies by begdat**&lt;/span&gt;, &#39;, &#39;&lt;span style=&quot;color:{colors[&quot;orange&quot;]}&quot;&gt;**number of companies by public_date**&lt;/span&gt;,&lt;br&gt;&#39;, &#39;&lt;span style=&quot;color:{colors[&quot;magenta&quot;]}&quot;&gt;**ideal number of companies proportional to delisting**&lt;/span&gt;&#39; ) ggplot() + geom_line( data = com.active.years, mapping = aes(x = year, y = number_of_companies, color = &quot;# companies by public_date&quot;), linewidth = 1 ) + geom_line( data = com.active.frequency, mapping = aes( x = year, y = proportion_wanted * sum(com.active.years$number_of_companies), color = &quot;ideal # companies proportional to delisting&quot; ), linewidth = 1 ) + geom_line( data = com.active.begdat, mapping = aes(x = year, y = number_of_companies, color = &quot;# companies by begdat&quot;), linewidth = 1 ) + scale_color_manual( breaks = c( &quot;# companies by begdat&quot;, &quot;# companies by public_date&quot;, &quot;ideal # companies proportional to delisting&quot; ), values = c( as.character(colors[&quot;blue&quot;]), as.character(colors[&quot;orange&quot;]), as.character(colors[&quot;magenta&quot;]) ) ) + labs( title = &quot;Number of active and listed companies&quot;, subtitle = subtitle, x = &quot;year of financial ratios&quot;, y = &quot;Frequency&quot; ) + theme( plot.subtitle = element_markdown(), legend.position = &quot;none&quot; ) ### Histograms of original vs imputed features The left histograms are based on features without imputation, the right histograms are based on imputed features. for (i in 1:nrow(com.features.fin)) { feature_description &lt;- com.features.fin[[i, 1]] feature_identifier &lt;- com.features.fin[[i, 2]] plot_without_imputation &lt;- hist_delisting_frequency( com.winsorized, feature_identifier, feature_description ) plot_with_imputation_pmm &lt;- hist_delisting_frequency( com.imputed %&gt;% left_join(com.winsorized %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date)), feature_identifier, feature_description ) print(plot_without_imputation | plot_with_imputation_pmm) } print(paste( &quot;Number of missing cells in original data set: &quot;, com %&gt;% summarize(n = sum(is.na(.))) )) ## [1] &quot;Number of missing cells in original data set: 543630&quot; print(paste( &quot;Number of missing cells in imputed data set: &quot;, com.imputed %&gt;% summarize(n = sum(is.na(.))) )) ## [1] &quot;Number of missing cells in imputed data set: 0&quot; "],["empirical-results.html", "3 Empirical Results 3.1 Random forest 3.2 Logistic Regression 3.3 Support Vector Machine 3.4 Neural net 3.5 Boosting 3.6 Ensemble", " 3 Empirical Results Metrics Contingency table observation prediction delisted prediction active delisted TP FN active FP TN \\[ \\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN} \\] \\[ \\text{Precision} = \\frac{\\text{# true positives}}{\\text{# predicted positives}} = \\frac{TP}{TP+FP} \\] \\[ \\text{Recall} = \\text{Sensitivity} = \\frac{\\text{# true positives}}{\\text{# actual positives}} = \\frac{TP}{TP+FN} \\] \\[F_1\\text{ score} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}} \\qquad \\text{(harmonic mean of precision and recall)} \\] \\[ \\text{Specificity} = \\frac{\\text{# true negatives}}{\\text{# actual negatives}} = \\frac{TN}{TN+FP} \\] \\[ \\text{Matthew&#39;s Correlation Coefficient} = \\text{MCC} = \\frac{TN \\cdot TP - FN \\cdot FP}{\\sqrt{(TP+FP)\\cdot (TP+FN)\\cdot (TN+FP)\\cdot (TN+FN)}} \\] Helper functions #&#39; computes performance metrics true positives (tp), false negatives (fn), #&#39; false positives (fp), true negatives (tn), accuracy, precision, recall, #&#39; F1 score (f1), specificity and Matthew&#39;s correlation coefficient (mcc) #&#39; #&#39; @param actuals vector containing {0,1} values representing the actual response #&#39; @param preds vector containing {0,1} values with predicted response #&#39; #&#39; @return list/tibble of computed metrics compute.metrics &lt;- function(actuals, preds) { if (is.factor(actuals)) { actuals = actuals == &quot;delisted&quot; } if (is.factor(preds)) { preds = preds == &quot;delisted&quot; } tp &lt;- sum(actuals == TRUE &amp; preds == TRUE) fn &lt;- sum(actuals == TRUE &amp; preds == FALSE) fp &lt;- sum(actuals == FALSE &amp; preds == TRUE) tn &lt;- sum(actuals == FALSE &amp; preds == FALSE) precision &lt;- tp / (tp + fp) recall &lt;- tp / (tp + fn) return (tibble(tp = tp, # TP fn = fn, # FN fp = fp, # FP tn = tn, # TN accuracy = (tp + tn) / (tp + fn + fp + tn), # accuracy precision = precision, # precision recall = recall, # recall / sensitivity f1 = 2 * (precision * recall) / (precision + recall), # F1 score specificity = tn / (tn + fp), # specificity mcc = (tn * tp - fn * fp) / sqrt((tp+fp))/sqrt((tp+fn))/sqrt((tn+fp))/sqrt((tn+fn)) # mcc ) ) } #&#39; computes performance metrics based on predicted probabilities for two classes. #&#39; optimal threshold for classification determined by MCC-F1 and ROC #&#39; optimality criteria #&#39; #&#39; @param data data frame containing actual response (obs) and predicted #&#39; probabilities for the positive class (delisted) #&#39; #&#39; @return vector with labelled performance metrics compute.summary &lt;- function (data, lev = NULL, model = NULL) { if (length(lev) &gt; 2) { stop(paste(&quot;Your outcome has&quot;, length(lev), &quot;levels.&quot;, &quot;The compute.summary() function isn&#39;t appropriate.&quot;)) } if (!all(levels(data[, &quot;pred&quot;]) == lev)) { stop(&quot;Levels of observed and predicted data do not match.&quot;) } # MCC-F1 cutpoint mccf1cutpoint &lt;- compute.mccf1cutpoint(data$delisted, data$obs) best_cutpoint.mccf1 &lt;- mccf1cutpoint$best_cutpoint metrics.mccf1 &lt;- compute.metrics(data$obs, data$delisted &gt;= best_cutpoint.mccf1) colnames(metrics.mccf1) &lt;- paste(colnames(metrics.mccf1), &quot;mccf1&quot;, sep = &quot;.&quot;) metrics.mccf1 &lt;- metrics.mccf1 %&gt;% mutate(mccf1.mccf1 = mccf1cutpoint$mccf1metric) # ROC cutpoint roc_auc &lt;- compute.roc(data$delisted, data$obs) best_cutpoint.roc &lt;- roc_auc$best_cutpoint metrics.roc &lt;- compute.metrics(data$obs, data$delisted &gt;= best_cutpoint.roc) colnames(metrics.roc) &lt;- paste(colnames(metrics.roc), &quot;roc&quot;, sep = &quot;.&quot;) metrics.roc &lt;- metrics.roc %&gt;% mutate(auc.roc = roc_auc$auc) out &lt;- unlist(cbind(metrics.roc, metrics.mccf1)) return (out) } #&#39; computes ROC curve for given predictions (binary classification) #&#39; #&#39; @param preds vector with predicted response #&#39; @param actuals vector representing the actual response #&#39; #&#39; @return plot of ROC curve, best classification threshold/cutpoint, #&#39; area under the curve (AUC), resulting contingency table compute.roc &lt;- function(preds, actuals) { if (is.factor(actuals)) { actuals = actuals == &quot;delisted&quot; } if (is.factor(preds)) { preds = preds == &quot;delisted&quot; } auc_roc.result &lt;- auc_roc(preds, actuals, returnDT = TRUE) if (length(auc_roc.result) == 1) { return (list(best_cutpoint = auc_roc.result, auc = 0.5)) } else { # best classification threshold best_cutpoint &lt;- auc_roc.result %&gt;% mutate(dist = sqrt((1 - CumulativeTPR)^2 + CumulativeFPR^2)) %&gt;% # Pythagoras (distance to (0,1)) filter(dist &lt;= min(dist)) plot.roc &lt;- ggplot(auc_roc.result, aes(x = CumulativeFPR, y = CumulativeTPR)) + geom_line(linewidth = 1) + geom_point(data = best_cutpoint, aes(CumulativeFPR, CumulativeTPR), size = 3, colour = &quot;blue&quot;) + coord_fixed() + labs( title = &quot;ROC curve&quot;, subtitle = &quot;Delisting predictor&quot;, x = &quot;False positive rate (1 - specificity)&quot;, y = &quot;True positive rate&quot; ) table.prediction &lt;- tibble( actuals = actuals, preds = preds &gt;= best_cutpoint$Pred ) con.table &lt;- table(table.prediction$actuals, table.prediction$preds) colnames(con.table) &lt;- c(&quot;prediction active&quot;, &quot;prediction delisted&quot;) row.names(con.table) &lt;- c(&quot;active&quot;, &quot;delisted&quot;) return (list(plot = plot.roc, best_cutpoint = best_cutpoint$Pred, best_cutpoint.text = paste(&quot;Classification threshold&quot;, formatC(best_cutpoint$Pred, digits = 4), &quot;minimizes distance of ROC curve to (0,1).&quot;), auc = tail(auc_roc.result$CumulativeArea, 1), con_table = con.table[2:1,2:1])) } } #&#39; computes MCCF1 curve for given predictions (binary classification) #&#39; #&#39; @param preds vector with predicted response #&#39; @param actuals vector representing the actual response #&#39; #&#39; @return plot of MCCF1 curve, best classification threshold/cutpoint, #&#39; value of MCCF1 metric, resulting contingency table compute.mccf1cutpoint &lt;- function(preds, actuals) { if (is.factor(actuals)) { actuals = actuals == &quot;delisted&quot; } if (is.factor(preds)) { preds = preds == &quot;delisted&quot; } mccf1.result &lt;- mccf1(preds, actuals) mccf1.summary &lt;- summary.mccf1(mccf1.result) # best threshold and mccf1 metric plot.mccf1 &lt;- ggplot(mccf1.result, aes(x = f1, y = mcc.nor)) + geom_path(linewidth = 1) + geom_line(data = mccf1.result %&gt;% filter(preds == mccf1.summary$best_threshold), aes(f1, mcc.nor), size = 3, colour = &quot;blue&quot;) + xlim(0,1) + ylim(0,1) + coord_fixed() + labs( title = &quot;MCC vs. F1-score&quot;, subtitle = &quot;Delisting predictor&quot;, x = &quot;F1 score&quot;, y = &quot;unit-normalized MCC&quot; ) table.prediction &lt;- tibble( actuals = actuals, preds = preds &gt;= as.numeric(mccf1.summary$best_threshold) ) con.table &lt;- table(table.prediction$actuals, table.prediction$preds) colnames(con.table) &lt;- c(&quot;prediction active&quot;, &quot;prediction delisted&quot;) row.names(con.table) &lt;- c(&quot;active&quot;, &quot;delisted&quot;) return (list(plot = plot.mccf1, best_cutpoint = mccf1.summary$best_threshold, best_cutpoint.text = paste(&quot;Classification threshold&quot;, formatC(as.numeric(mccf1.summary$best_threshold), digits = 4), &quot;maximizes MCCF1 metric.&quot;), mccf1metric = mccf1.summary$mccf1_metric, con_table = con.table)) } #&#39; computes MCCF1 curve for given predictions (binary classification) #&#39; #&#39; @param preds vector with predicted response #&#39; @param actuals vector representing the actual response #&#39; @param mccf1cutpoint true if computation uses MCCF1 curve, false if ROC curve #&#39; #&#39; @return best classification cutpoint | additionally plot of curve, #&#39; value of MCCF1/AUC metric, resulting contingency table compute.cutpoint &lt;- function(preds, actuals, mccf1cutpoint = FALSE) { if (mccf1cutpoint) { return (compute.mccf1cutpoint(preds, actuals)) } else { return (compute.roc(preds, actuals)) } } Helper function to display variable importance #&#39; visualize 20 most important features #&#39; #&#39; @param data data frame containing feature name and relative variable #&#39; importance (vimp) value for each feature of interest #&#39; @param subtitle subtitle of plot #&#39; #&#39; @return barplot of the 20 most important features plot.vimp &lt;- function(data, subtitle) { data %&gt;% arrange(desc(vimp)) %&gt;% filter(row_number() &lt;= 20) %&gt;% ggplot( aes(x = reorder(feature, -vimp), y = vimp) ) + geom_bar(stat = &quot;identity&quot;) + coord_flip() + geom_text(aes(label = formatC(vimp / max(vimp), digits = 2)), hjust = 1, colour = &quot;white&quot; ) + labs( title = &quot;Variable Importance (VIMP)&quot;, subtitle = subtitle, y = &quot;relative feature importance&quot;, x = &quot;Features&quot; ) } Helper functions to illustrate performance metrics #&#39; prints hyperparameters, plots, cutpoints and metrics of ROC and MCC-F1 curve #&#39; #&#39; @param hyperparameters data frame containing hyperparameters of model of interest #&#39; @param actuals vector representing the actual response #&#39; @param pred.probability vector containing predicted probabilities of positive #&#39; class illustrate.metrics &lt;- function(hyperparameters, actuals, pred.probability) { pander(hyperparameters, caption = &quot;Hyperparameters of the selected model&quot;) # Determine best cutpoint based on AUC cutpoint.roc &lt;- compute.roc(pred.probability, actuals) print(cutpoint.roc$plot) print(cutpoint.roc$best_cutpoint.text) pander(cutpoint.roc$con_table, caption = &quot;Classification table with ROC cutpoint&quot;) # Determine best cutpoint which maximizes MCCF1-score cutpoint.mccf1 &lt;- compute.mccf1cutpoint(pred.probability, actuals) print(cutpoint.mccf1$plot) print(cutpoint.mccf1$best_cutpoint.text) pander(cutpoint.mccf1$con_table, caption = &quot;Classification table with MCCF1 cutpoint&quot;) # Display metrics for cutpoint rules ROC and MCCF1 metrics.table &lt;- rbind(compute.metrics(actuals, pred.probability &gt;= as.numeric(cutpoint.roc$best_cutpoint))) metrics.table &lt;- rbind(metrics.table, compute.metrics(actuals, pred.probability &gt;= as.numeric(cutpoint.mccf1$best_cutpoint))) metrics.table &lt;- cbind(c(&quot;ROC&quot;, &quot;MCCF1&quot;), metrics.table) pander(metrics.table) return (list(cutpoint.roc = cutpoint.roc$best_cutpoint, cutpoint.mccf1 = cutpoint.mccf1$best_cutpoint)) } #&#39; creates boxplots for performance metrics of models fitted with k-fold cross- #&#39; validation #&#39; #&#39; @param resamples.mccf1 data frame with performance metrics for models on a #&#39; k-fold subset using MCCF1 cutpoint for classification #&#39; @param resamples.roc data frame with performance metrics for models on a #&#39; k-fold subset using ROC cutpoint for classification #&#39; @param model_identifier technical identifier of model name illustrate.metrics.boxplot &lt;- function(resamples.mccf1, resamples.roc, model_identifier) { rf.metrics.cv.mccf1 &lt;- resamples.mccf1 %&gt;% select(c(accuracy.mccf1, precision.mccf1, recall.mccf1, f1.mccf1, specificity.mccf1, mcc.mccf1)) %&gt;% pivot_longer( cols = everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot; ) %&gt;% mutate( model = model_identifier, group = &quot;mccf1&quot;, metric = sub(&quot;\\\\.(.*)&quot;, &quot;&quot;, metric) ) rf.metrics.cv.roc &lt;- resamples.roc %&gt;% select(c(accuracy.roc, precision.roc, recall.roc, f1.roc, specificity.roc, mcc.roc)) %&gt;% pivot_longer( cols = everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot; ) %&gt;% mutate( model = model_identifier, group = &quot;roc&quot;, metric = sub(&quot;\\\\.(.*)&quot;, &quot;&quot;, metric) ) plot &lt;- ggplot(rbind(rf.metrics.cv.mccf1, rf.metrics.cv.roc), aes(x = metric, y = value, col = group)) + geom_boxplot() + labs( title = &quot;Best model&quot; ) return (plot) } Helper function for MCCF1 curve computations #&#39; computes MCCF1 metric #&#39; #&#39; @param preds vector containing {0,1} binary predictions #&#39; @param actuals vector representing the actual responses out of {0,1} #&#39; #&#39; @return MCCF1 metric mccf1 &lt;- function(preds, actuals){ result &lt;- tibble( preds = preds, actuals = actuals ) %&gt;% arrange(preds) %&gt;% mutate( sumPositives = sum(actuals == 1), sumNegatives = sum(actuals == 0), cumulativeFN = cumsum(actuals == 1), cumulativeTN = cumsum(actuals == 0), cumulativeTP = sumPositives - cumulativeFN, cumulativeFP = sumNegatives - cumulativeTN, precision = ifelse(cumulativeTP + cumulativeFP &gt; 0, cumulativeTP / (cumulativeTP + cumulativeFP), 0), # precision recall = ifelse(cumulativeTP + cumulativeFN &gt; 0, cumulativeTP / (cumulativeTP + cumulativeFN), 0), # recall / sensitivity f1 = ifelse(precision + recall &gt; 0, 2 * (precision * recall) / (precision + recall), 0), # F-score mcc = (cumulativeTN * cumulativeTP - cumulativeFN * cumulativeFP) / sqrt((cumulativeTP+cumulativeFP))/sqrt((cumulativeTP+cumulativeFN))/sqrt((cumulativeTN+cumulativeFP))/sqrt((cumulativeTN+cumulativeFN)) ) %&gt;% mutate(mcc.nor = (mcc + 1) / 2) return(result) } # based on https://github.com/hoffmangroup/mccf1/blob/master/mccf1.R summary.mccf1 &lt;- function(object, digits, bins = 100, ...){ # get rid of NaN values in the vectors of mcc.nor and F1 mcc.nor_truncated &lt;- object$mcc.nor[2: (length(object$mcc.nor) - 1)] f_truncated &lt;- object$f1[2: (length(object$f1) - 1)] # get the index of the point with largest normalized MCC (&quot;point&quot; refers to the point on the MCC-F1 curve) index_of_max_mcc &lt;- which.max(mcc.nor_truncated) # define points on the MCC-F1 curve located on the left of the point with the highest normalized MCC as &quot;left curve&quot; # get the left curve by getting the subvectors of MCC and F1 up to the index of the largest normalized MCC mcc_left &lt;- mcc.nor_truncated[1: index_of_max_mcc] f_left &lt;- f_truncated[1: index_of_max_mcc] # define points on the MCC-F1 curve located on the right of the point with the highest normalized MCC as &quot;right curve&quot; # get the right curve by getting the subvectors of MCC and F1 after the index of the largest normalized MCC mcc_right &lt;- mcc.nor_truncated[(index_of_max_mcc + 1): length(mcc.nor_truncated)] f_right &lt;- f_truncated[(index_of_max_mcc + 1): length(f_truncated)] # divide the range of normalized MCC into subranges unit_len &lt;- (max(mcc.nor_truncated) - min(mcc.nor_truncated)) / bins # calculate the sum of mean distances from the left curve to the point (1, 1) mean_distances_left &lt;- 0 for (i in 1: bins){ # find all the points on the left curve with normalized MCC between unit_len*(i-1) and unit_len*i pos1 &lt;- which(mcc_left &gt;= min(mcc.nor_truncated) + (i-1) * unit_len) pos2 &lt;- which(mcc_left &lt;= min(mcc.nor_truncated) + i * unit_len) pos &lt;- c() for (index in pos1){ if (index %in% pos2){ pos &lt;- c(pos, index) } } sum_of_distance_within_subrange &lt;- 0 for (index in pos){ d &lt;- sqrt((mcc_left[index] - 1)^2 + (f_left[index] - 1)^2) sum_of_distance_within_subrange &lt;- sum_of_distance_within_subrange + d } mean_distances_left &lt;- c(mean_distances_left, sum_of_distance_within_subrange / length(pos)) } # get rid of NAs in mean_distances_left and sum the mean distances num_of_na_left &lt;- sum(is.na(mean_distances_left)) sum_of_mean_distances_left_no_na &lt;- sum(mean_distances_left, na.rm = T) # calculate the sum of mean distances from the right curve to the point (1, 1) mean_distances_right &lt;- 0 for (i in 1: bins){ # find all the points on the right curve with normalized MCC between unit_len*(i-1) and unit_len*i pos1 &lt;- which(mcc_right &gt;= min(mcc.nor_truncated) + (i-1) * unit_len) pos2 &lt;- which(mcc_right &lt;= min(mcc.nor_truncated) + i * unit_len) pos &lt;- c() for (index in pos1){ if (index %in% pos2){ pos &lt;- c(pos, index) } } sum_of_distance_within_subrange &lt;- 0 for (index in pos){ d &lt;- sqrt((mcc_right[index] - 1)^2 + (f_right[index] - 1)^2) sum_of_distance_within_subrange &lt;- sum_of_distance_within_subrange + d } mean_distances_right &lt;- c(mean_distances_right, sum_of_distance_within_subrange / length(pos)) } # get rid of NAs in mean_distances_right and sum the mean distances num_of_na_right &lt;- sum(is.na(mean_distances_right)) sum_of_mean_distances_right_no_na &lt;- sum(mean_distances_right, na.rm = T) # calculate the MCC-F1 metric mccf1_metric &lt;- 1 - ((sum_of_mean_distances_left_no_na + sum_of_mean_distances_right_no_na) / (bins*2 - num_of_na_right - num_of_na_left)) / sqrt(2) # find the best threshold best_threshold &lt;- object %&gt;% mutate(dist = sqrt((1 - f1)^2 + (1-mcc.nor)^2)) %&gt;% # Pythagoras (distance to (1,1)) filter(dist &lt;= min(dist, na.rm = TRUE)) # output of the function is the MCC-F1 metric and the top threshold mccf1_result &lt;- data.frame(mccf1_metric = mccf1_metric, best_threshold = best_threshold$preds[1]) return(mccf1_result) } 3.1 Random forest Feature names for random forest model pander(names(data.training %&gt;% select(-permno, -public_date))) capital_ratio, at_turn, inv_turn, rect_turn, pay_turn, cash_ratio, curr_ratio, aftret_eq, aftret_equity, gpm, opmad, roa, roe, de_ratio, debt_assets, pe_exi, response, capital_ratio_h, at_turn_h, inv_turn_h, rect_turn_h, pay_turn_h, cash_ratio_h, curr_ratio_h, aftret_eq_h, aftret_equity_h, gpm_h, opmad_h, roa_h, roe_h, de_ratio_h, debt_assets_h, pe_exi_h, gdpc1_chg, cpiaucsl_chg, fredfunds_chg and sp500_chg The random forest implementation randomForestSRC provides three hyperparameters. ntree: number of trees (default: 500) nodesize: minimum size of terminal node (default: 5) mtry: number of variables to possibly split at each node (default: number of variables divided by 3) Find optimal mtry and nodesize tuning parameter using standardized out-of-sample error if (REDO_MODEL_FITTING) { set.seed(1) # finds optimal mtry and nodesize tuning parameter for a random forest using # standardized out-of-sample-error rfr.reg.1000.tuned &lt;- tune( response ~ ., as.data.frame(data.training %&gt;% mutate(response = ifelse(response == &quot;delisted&quot;, 1, 0)) %&gt;% select(-permno, -public_date)), mtryStart = sqrt(ncol(data.training) - 3), # number of covariates used for each tree nodesizeTry = c(1:9, seq(10, 100, by = 5)), # minimum size of terminal node ntreeTry = 1000, sampsize = function(x) { min(x * .632, max(150, x^(3 / 4))) }, # sampling without replacement nsplit = 10, stepFactor = 1.25, # at each iteration, mtry is inflated by this value improve = 1e-3, strikeout = 5, maxIter = 25, trace = FALSE, doBest = TRUE ) save(rfr.reg.1000.tuned, file = &quot;../data/rfr.reg.1000.tuned.RData&quot;) } else { load(&quot;../data/rfr.reg.1000.tuned.RData&quot;) } # contour plot of standardized out-of-sample errors for different nodesize and mtry plot.tune &lt;- function(o, linear = TRUE) { nodesize &lt;- o$results[, 1] mtry &lt;- o$results[, 2] oob_error &lt;- o$results[, 3] # so &lt;- interp(x=x, y=y, z=z, linear = linear) idx &lt;- which.min(oob_error) x0 &lt;- nodesize[idx] y0 &lt;- mtry[idx] ggplot(data.frame(nodesize = nodesize, mtry = mtry, oob_error = oob_error)) + geom_contour_filled(mapping = aes(nodesize, mtry, z = oob_error)) + labs( title = &quot;Error rate for nodesize and mtry&quot;, subtitle = &quot;OOB errors&quot; ) } plot.tune(rfr.reg.1000.tuned) The contour plot shows a standardized out-of-sample error for random forests with 1000 trees and different nodesize and mtry. The lowest errors are observed for small nodesizes and large mtry. Metrics for varying nodesize and mtry optimization.nodesize.mtry &lt;- tibble( nodesize = numeric(), mtry = numeric(), samples = character(), tp = numeric(), fn = numeric(), fp = numeric(), tn = numeric(), accuracy = numeric(), precision = numeric(), recall = numeric(), fscore = numeric() ) if (REDO_MODEL_FITTING) { for (nodesize in c(1:14, seq(15, 100, by = 5))) { for (mtry in seq(ceiling(sqrt(ncol(data.training))), ncol(data.training)*0.7, by = 5)) { print(paste(&quot;nodesize:&quot;, nodesize, &quot;| mtry:&quot;, mtry)) rfr.nodesize.mtry.result &lt;- rfsrc( response ~ ., data = as.data.frame(data.training %&gt;% mutate(response = ifelse(response == &quot;delisted&quot;, 1, 0)) %&gt;% select(-permno, -public_date)), nodesize = nodesize, ntree = 1000, mtry = mtry, statistics = TRUE, save.memory = TRUE ) train_prediction_table &lt;- tibble( actual = (data.training %&gt;% mutate(response = ifelse(response == &quot;delisted&quot;, 1, 0)))$response, predicted.inb = rfr.nodesize.mtry.result$predicted, predicted.oob = rfr.nodesize.mtry.result$predicted.oob ) auc_roc.result &lt;- auc_roc(train_prediction_table$predicted.inb, train_prediction_table$actual, returnDT=TRUE) # best classification threshold best_cutpoint &lt;- auc_roc.result %&gt;% ####################################### # Pythagoras (distance to (0,1)) mutate(dist = sqrt((1 - CumulativeTPR)^2 + CumulativeFPR^2)) %&gt;% filter(dist &lt;= min(dist)) train_prediction_table &lt;- train_prediction_table %&gt;% mutate( result.inb = predicted.inb &gt; best_cutpoint$Pred, result.oob = predicted.oob &gt; best_cutpoint$Pred ) optimization.nodesize.mtry &lt;- rbind( optimization.nodesize.mtry, append( compute.metrics(train_prediction_table$actual == 1, train_prediction_table$result.inb), list(nodesize = nodesize, mtry = mtry, samples = 0), after = 0)) optimization.nodesize.mtry &lt;- rbind(optimization.nodesize.mtry, append( compute.metrics(train_prediction_table$actual == 1, train_prediction_table$result.oob), list(nodesize = nodesize, mtry = mtry, samples = 1), after = 0)) } } colnames(optimization.nodesize.mtry) &lt;- c(&quot;nodesize&quot;, &quot;mtry&quot;, &quot;samples&quot;, &quot;TP&quot;, &quot;FN&quot;, &quot;FP&quot;, &quot;TN&quot;, &quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;, &quot;specificity&quot;, &quot;mcc&quot;) # convert binary variable to INB and OOB factors optimization.nodesize.mtry &lt;- optimization.nodesize.mtry %&gt;% mutate(samples = factor(samples, levels = c(0, 1), labels = c(&quot;INB&quot;, &quot;OOB&quot;))) save(optimization.nodesize.mtry, file = &quot;../data/optimization.nodesize.mtry.RData&quot;) } else { load(&quot;../data/optimization.nodesize.mtry.RData&quot;) } plot.nodesize.metric = function (data, metric, mtry.fix) { plot &lt;- ggplot(data = data %&gt;% filter(mtry == mtry.fix), aes(x = nodesize, col = samples, linetype = samples)) + geom_line(aes_string(y = metric), linewidth = 1) + labs( title = paste(&quot;Nodesize vs&quot;, metric), subtitle = paste(&quot;RF: ntree = 1000 and mtry =&quot;, mtry.fix), x = &quot;nodesize&quot;, y = metric, colour = &quot;Evaluation type&quot;, linetype = &quot;Evaluation type&quot; ) } plot.mtry.metric &lt;- function (data, metric, nodesize.fix) { plot &lt;- ggplot(data = data %&gt;% filter(nodesize == nodesize.fix), aes(x = mtry, col = samples, linetype = samples)) + geom_line(aes_string(y = metric), linewidth = 1) + labs( title = paste(&quot;Mtry vs&quot;, metric), subtitle = paste(&quot;RF: ntree = 1000 and nodesize =&quot;, nodesize.fix), x = &quot;mtry&quot;, y = metric, colour = &quot;Evaluation type&quot;, linetype = &quot;Evaluation type&quot; ) } metrics &lt;- c(&quot;accuracy&quot;, &quot;precision&quot;, &quot;recall&quot;, &quot;f1&quot;) plot.nodesize.metrics.list &lt;- lapply(metrics, plot.nodesize.metric, data = optimization.nodesize.mtry, mtry.fix = 17) plot.mtry.metrics.list &lt;- lapply(metrics, plot.mtry.metric, data = optimization.nodesize.mtry, nodesize.fix = 15) plot.nodesize.metrics &lt;- ggarrange(plot.nodesize.metrics.list[[1]], plot.nodesize.metrics.list[[2]], plot.nodesize.metrics.list[[3]], plot.nodesize.metrics.list[[4]], nrow = 2, ncol = 2, legend = &quot;bottom&quot;, common.legend = TRUE) plot.mtry.metrics &lt;- ggarrange(plot.nodesize.metrics.list[[1]], plot.nodesize.metrics.list[[2]], plot.nodesize.metrics.list[[3]], plot.nodesize.metrics.list[[4]], plot.mtry.metrics.list[[1]], plot.mtry.metrics.list[[2]], plot.mtry.metrics.list[[3]], plot.mtry.metrics.list[[4]], nrow = 2, ncol = 4, legend = &quot;bottom&quot;, common.legend = TRUE) plot.mtry.metrics Parameter choice for random forest ntree: 1000 nodesize: 5 mtry: 12 # function and object definitions for custom model in caret package # https://topepo.github.io/caret/using-your-own-model-in-train.html rf.parameters &lt;- data.frame( parameter = c(&quot;ntree&quot;, &quot;nodesize&quot;, &quot;mtry&quot;), class = rep(&quot;numeric&quot;, 3) ) rf.grid &lt;- function(x, y, len = NULL, search = &quot;grid&quot;) { } rf.fit &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) { rfr.result &lt;- rfsrc( y ~ ., data = data.frame(x, y = as.factor(y)), ntree = param$ntree, nodesize = param$nodesize, mtry = param$mtry, statistics = TRUE, save.memory = TRUE ) return (rfr.result) } rf.predict &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) { prediction &lt;- randomForestSRC::predict.rfsrc(modelFit, as.data.frame(newdata))$class return (prediction) } rf.predict.prob &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL) { prediction &lt;- randomForestSRC::predict.rfsrc(modelFit, as.data.frame(newdata))$predicted return (prediction)#data.frame(active = 1 - prediction, delisted = prediction)) } rf.list = list( type = c(&quot;Classification&quot;, &quot;Regression&quot;), method = &quot;randomForestSRC&quot;, library = c(&quot;randomForestSRC&quot;), parameters = rf.parameters, grid = rf.grid, fit = rf.fit, predict = rf.predict, prob = rf.predict.prob, sort = function(x) x ) Model fit if (REDO_MODEL_FITTING) { ### control the computational nuances of the train function #################### rf.fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, ## Estimate class probabilities classProbs = TRUE, ## Evaluate performance using ## the following function summaryFunction = compute.summary, returnResamp = &quot;all&quot;) ### fit model over different tuning parameters ################################# set.seed(1) rf.fit &lt;- caret::train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = rf.list, trControl = rf.fitControl, tuneGrid = expand.grid( ntree = 1000, nodesize = 5, mtry = 12), verbose = TRUE, metric = &quot;mcc.mccf1&quot; # specify which metric to optimize ) save(rf.fit, file = &quot;../data/rf.fit.RData&quot;) } else { load(&quot;../data/rf.fit.RData&quot;) } Model information and evaluation on training set rf.bestTune.mccf1 &lt;- rf.fit$resample %&gt;% group_by(ntree, nodesize, mtry) %&gt;% summarize(mccf1 = mean(mcc.mccf1), .groups = &quot;drop&quot;) %&gt;% slice(which.max(mccf1)) rf.bestTune.roc &lt;- rf.fit$resample %&gt;% group_by(ntree, nodesize, mtry) %&gt;% summarize(auc = mean(auc.roc), .groups = &quot;drop&quot;) %&gt;% slice(which.max(auc)) # Hyperparameters of best models for MCCF1 and ROC cutpoints hyperparameters.table &lt;- tibble( cutpoint = c(&quot;MCCF1&quot;, &quot;ROC&quot;), ntree = c(rf.bestTune.mccf1$ntree, rf.bestTune.roc$ntree), nodesize = c(rf.bestTune.mccf1$nodesize, rf.bestTune.roc$nodesize), mtry = c(rf.bestTune.mccf1$mtry, rf.bestTune.roc$mtry) ) pred.probability &lt;- predict(rf.fit, newdata = data.training, type = &quot;prob&quot;) rf.metrics &lt;- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted) ## [1] &quot;Classification threshold 0.2464 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.3198 maximizes MCCF1 metric.&quot; illustrate.metrics.boxplot(rf.fit$resample %&gt;% filter(ntree == hyperparameters.table$ntree[1] &amp; nodesize == hyperparameters.table$nodesize[1] &amp; mtry == hyperparameters.table$mtry[1]), rf.fit$resample %&gt;% filter(ntree == hyperparameters.table$ntree[2] &amp; nodesize == hyperparameters.table$nodesize[2] &amp; mtry == hyperparameters.table$mtry[2]), &quot;rf&quot;) Performance evaluation on test set # prediction scores on test data: (&quot;active&quot;, &quot;delisted&quot;) pred.test.probability &lt;- predict(rf.fit, newdata = data.test, type = &quot;prob&quot;) # metrics on test set using best threshold determined based on train data rf.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test.probability$delisted &gt;= rf.metrics$cutpoint.roc), compute.metrics(data.test$response, pred.test.probability$delisted &gt;= rf.metrics$cutpoint.mccf1)) %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(rf.summary.test, caption = &quot;Random forest on test data&quot;) Random forest on test data (continued below) splitting tp fn fp tn accuracy precision recall f1 ROC 320 116 425 1357 0.7561 0.4295 0.7339 0.5419 MCCF1 265 171 240 1542 0.8147 0.5248 0.6078 0.5632 specificity mcc 0.7615 0.4169 0.8653 0.4484 Variable importance Definition Permutation (Breiman-Cutler) Importance: In the OOB cases for a tree, randomly permute all values of the j-th variable. Put these new covariate values down the tree and compute a new internal error rate. The amount by which this new error exceeds the original OOB error is defined as the importance of the j-th variable for the tree. Averaging over the forest yields variable importance (VIMP). The following chunk determines the variable importance of all the features in the model data set using the permutation importance method. The twenty features with the highest variable importance are displayed in the barplot. rfr.vimp.results &lt;- vimp(rf.fit$finalModel, importance = &quot;permute&quot;) rfr.vimp &lt;- tibble( feature = rfr.vimp.results$xvar.names, vimp = 100 * rfr.vimp.results$importance[,&quot;all&quot;] ) %&gt;% arrange(desc(vimp)) plot.vimp(rfr.vimp, &quot;Permutation (Beiman-Cutler) Importance&quot;) The price-to-earnings ratio feature pe_exi is the most important financial ratio feature. The histogram with delisting frequency for pe_exi indicates indeed high predictive power between the pe_exi values near zero and the relative delisting frequency. The financial ratio features aftret_equity, opmad and `roe`` follow in the importance ranking. This is plausible as the histograms with delisting frequency (see Exploratory Data Analysis section) indicated predictive power for delisting. If the (after-tax) return on equity is low and/or the operative profit margin is negative, then the delisting frequency is relatively high. Marginal plots Definition Partial Dependence Function: Let \\(F(x)\\) be the target function in a supervised problem where \\(x = (x_1, \\dots, x_p)\\). Let \\(x_s\\) denote \\(x\\) restricted to coordinate indices \\(s \\subset \\{1,\\dots,p\\}\\). Likewise using the notation \\(\\backslash s=\\{1,‚Ä¶,p\\}\\backslash s\\) to denote the complement of \\(s\\), let \\(x‚àñbackslash s\\) denote the coordinates of \\(x\\) with indices not in \\(s\\). The (marginal) partial dependence function is \\[ \\overline{F}(x_s) = \\int F(x_s,x_{\\backslash s}) \\cdot p_{\\backslash s}(x_{\\backslash s})dx_{\\backslash s} \\] where \\(p_{\\backslash s}(x_{\\backslash s}) = \\int p(x) dx_{\\backslash s}\\) is the marginal probability density of \\(x_{\\backslash s}\\). The target function \\(F(x)\\) is usually not known for supervised learning problems. In a random forest, the target function is estimated by an ensemble of trees. Let \\(\\hat{F}\\) denote this estimator. Let \\(X_1,\\dots, X_n\\) denote the features from the learning data, then the estimated partial dependence function is \\[ \\hat{\\overline{F}}(x_s) = \\frac{1}{n} \\sum_{i=1}^n \\hat{F}(x_s, X_{i,\\backslash s}) \\] The plots of the estimated marginal partial dependence function for the twenty most important features are displayed below. if (REDO_MODEL_FITTING) { rf.partial_plots = list() counter = 1 for (feature_identifier in (rfr.vimp %&gt;% filter(row_number() &lt;= 20))$feature) { min &lt;- min(rf.fit$finalModel$xvar[feature_identifier]) max &lt;- max(rf.fit$finalModel$xvar[feature_identifier]) sequence_values &lt;- seq(min, max, length.out = 30) # partial effect for feature feature_identifier partial.obj &lt;- partial(rf.fit$finalModel, partial.xvar = feature_identifier, partial.values = sequence_values ) ## helper function for extracting the partial effects pdta &lt;- get.partial.plot.data(partial.obj) rf.partial_plots[[counter]] &lt;- tibble(x = pdta$x, y = pdta$yhat) counter = counter + 1 } save(rf.partial_plots, file = &quot;../data/rf.partial_plots.RData&quot;) } else { load(&quot;../data/rf.partial_plots.RData&quot;) } counter = 1 # marginal plots of 20 most important features on test data for (feature_identifier in (rfr.vimp %&gt;% filter(row_number() &lt;= 20))$feature) { plot &lt;- hist_delisting_frequency( (data.test %&gt;% inner_join(com %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))), feature_identifier, feature_identifier, marginal.data = pred.test.probability$delisted ) print(plot) # marginal plots of 20 most important features and feature pe_exi on test data plot &lt;- ggplot( data = rf.partial_plots[[counter]], aes(x = x, y = y) ) + geom_point() + geom_smooth() + labs( title = &quot;Partial dependence plot&quot;, subtitle = &quot;Random forest&quot;, x = feature_identifier, y = &quot;Prediction&quot; ) print(plot) counter = counter + 1 } pe_exi1 and pe_exi2: If the price-to-earnings ratio (per share) is around zero (market value per share around zero), more delistings occur than with an in absolute terms larger price-to-earnings ratio. If the PE ratio is negative, the market value of a share is large compared to a small negative loss. If the PE ratio is positive, the market value per share is lower than the earnings per share which signifies underpricing of the share. cash_ratio1, quick_ratio1: If the ratio between cash (plus short-term investments) and liabilities increases, the partial dependence function decreases. opmbd1, opmbd2: The observed and modeled delisting frequency is relatively high for negative operative profit margins. The frequency drops the more the operative profit margin gets positive. roe1: If the returns on equity are negative, the observed and modeled delisting frequency is higher thant for positive values. aftret_equity: If the net income over total shareholder‚Äôs equity is negative, the observed and modeled delisting frequency is higher than for positive ratios. fredfund_chg: The the Federal Reserve decreases the federal funds rate in periods of economic crisis. In these times, relatively many companies delist. In times of good conjuncture, the Federal Reserve rises interest rates. (Will this result in problems for the years 2021-2023?) roe_h: The random forest models a higher delisting frequency for companies whose return on equity decreased than for companies whose return on equity increased in the past up to seven years. at_turn_h: If the asset turnover declined over the past seven years cpiaucsl: The modeled delisting frequency is the highest for negative or small positive CPI rate changes. There is a decrease followed by a global minimum in modeled delisting between an annual inflation rate of 0.06 and 0.09. For CPI rate changes higher than 0.09, the modeled delisting frequency increases. cash_ratio_h, quick_ratio_h: If the cash ratio increased in the past, the modeled delisting frequency is lower than for a decrease of the ratio in the past. aftret_equity_h, aftret_eq_h: If the net income to equity ratio increased in the past, the modeled delisting frequency is lower than for a decrease of the ratio in the past. roe_h, roa_h: ????????????????? debt_assets_h: ???????????? pay_turn_h: Cost of goods sold / Average of Accout payables pe_exi_h: If the PE ratio increased in the past, the gdpc1_chg: If the gross domestic product change is large, delisting is relatively high. R packages for neural networks, SVN? Further models possibly coming soon: Logistic Regression with Lasso regularization, Neural Network, SVM, Boosting 3.2 Logistic Regression \\[ \\underset{(\\beta_0,\\beta)\\in R^{(p+1)}}{\\min} -\\left[ \\frac{1}{N} \\sum_{i=1}^N y_i \\cdot (\\beta_0 + x_i^T\\beta) - \\log(1 + e^{\\beta_0+x_i^T\\beta}) \\right] + \\lambda \\left[\\frac{1-\\alpha}{2} \\lVert \\beta \\rVert_2 + \\alpha \\lVert \\beta \\rVert_1 \\right] \\] Model fit if (REDO_MODEL_FITTING) { ### control the computational nuances of the train function ################## lr.fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, # estimate class probabilities # evaluate performance using the following function summaryFunction = compute.summary, returnResamp = &quot;all&quot;) ### fit model over different tuning parameters ############################### lr.fit &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;glmnet&quot;, trControl = lr.fitControl, tuneGrid = expand.grid( alpha = seq(0, 1, by = 0.1), lambda = 10^(-seq(1, 6, by = 1)) ), verbose = TRUE, metric = &quot;mcc.mccf1&quot;) # specify which metric to optimize save(lr.fit, file = &quot;../data/lr.fit.RData&quot;) } else { load(&quot;../data/lr.fit.RData&quot;) } ggplot(lr.fit) + labs( title = &quot;Logistic regression with elastic net regularization&quot;, subtitle = &quot;MCC vs. mixing percentage alpha&quot;, x = &quot;Mixing percentage alpha&quot;, y = &quot;MCC (10-fold cross-validation)&quot; ) Model information and evaluation on training set lr.bestTune.mccf1 &lt;- lr.fit$resample %&gt;% group_by(alpha, lambda) %&gt;% summarize(mccf1 = mean(mcc.mccf1), .groups = &quot;drop&quot;) %&gt;% slice(which.max(mccf1)) lr.bestTune.roc &lt;- lr.fit$resample %&gt;% group_by(alpha, lambda) %&gt;% summarize(auc = mean(auc.roc), .groups = &quot;drop&quot;) %&gt;% slice(which.max(auc)) # Hyperparameters of best models for MCC-F1 and ROC cutpoints hyperparameters.table &lt;- tibble( cutpoint = c(&quot;MCCF1&quot;, &quot;ROC&quot;), alpha = c(lr.bestTune.mccf1$alpha, lr.bestTune.roc$alpha), lambda = c(lr.bestTune.mccf1$lambda, lr.bestTune.roc$lambda), ) pred.probability &lt;- predict(lr.fit, newdata = data.training, type = &quot;prob&quot;) lr.metrics &lt;- illustrate.metrics(hyperparameters.table, data.test$response, pred.test.probability$delisted) ## [1] &quot;Classification threshold 0.2439 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.3142 maximizes MCCF1 metric.&quot; illustrate.metrics.boxplot(lr.fit$resample %&gt;% filter(alpha == hyperparameters.table$alpha[1] &amp; lambda == hyperparameters.table$lambda[1]), lr.fit$resample %&gt;% filter(alpha == hyperparameters.table$alpha[2] &amp; lambda == hyperparameters.table$lambda[2]), &quot;lr&quot;) Performance evaluation on test set # MCCF1 variant: prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) pred.test.probability &lt;- predict(lr.fit, newdata = data.test, type = &quot;prob&quot;) # ROC variant: fit model using best hyperparameters and # find best threshold and # prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) set.seed(1) lr.fit.roc &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;glmnet&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = compute.summary), tuneGrid = data.frame( alpha = lr.bestTune.roc$alpha, lambda = lr.bestTune.roc$lambda ), verbose = TRUE, metric = &quot;auc.roc&quot;) # specify which metric to optimize lr.metrics.roc.cutpoint &lt;- compute.roc( predict(lr.fit.roc, newdata = data.training, type = &quot;prob&quot;)$delisted, data.training$response)$best_cutpoint pred.test.probability.roc &lt;- predict(lr.fit.roc, newdata = data.test, type = &quot;prob&quot;) # metrics on test set using best threshold determined based on train data lr.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted &gt;= lr.metrics.roc.cutpoint), compute.metrics(data.test$response, pred.test.probability$delisted &gt;= lr.metrics$cutpoint.mccf1)) %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(lr.summary.test, caption = &quot;Logistic regression on test data&quot;) Logistic regression on test data (continued below) splitting tp fn fp tn accuracy precision recall f1 ROC 282 154 516 1266 0.6979 0.3534 0.6468 0.4571 MCCF1 137 299 157 1625 0.7944 0.466 0.3142 0.3753 specificity mcc 0.7104 0.2958 0.9119 0.265 Variable importance lr.vimp.results &lt;- varImp(lr.fit)$importance lr.vimp &lt;- lr.vimp.results %&gt;% mutate(feature = rownames(lr.vimp.results), vimp = Overall) %&gt;% arrange(desc(vimp)) plot.vimp(lr.vimp, &quot;Importance based on value of t-statistic for each model parameter&quot;) Marginal plots # marginal plots of 20 most important features and feature pe_exi on test data for (feature_identifier in c((lr.vimp %&gt;% filter(row_number() &lt;= 20))$feature, &quot;pe_exi&quot;)) { plot &lt;- hist_delisting_frequency( (data.test %&gt;% inner_join(com %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))), feature_identifier, feature_identifier, marginal.data = pred.test.probability$delisted ) print(plot) } 3.3 Support Vector Machine Let the training data consist of \\(N\\) pairs \\((x_1,y_1),(x_2,y_2),\\ldots,(x_N,y_N)\\), with \\(x_i \\in \\mathbb{R}^p\\) and \\(y_i\\in\\{0,1\\}\\). Define a hyperplane by \\[ \\{x:f(x)=x^T\\beta+\\beta_0=0\\} \\] where \\(\\beta\\) is a vector. \\[ \\begin{align*} \\hat{f}(x) &amp;= \\text{sign} \\left( h(x)^T \\hat{\\beta} + \\hat{\\beta_0} \\right) = \\text{sign} \\left( h(x)^T \\left( \\sum_{i=1}^N \\hat{\\alpha_i} y_i h(x_i) \\right) + \\hat{\\beta_0} \\right) \\\\ &amp;= \\text{sign} \\left( \\sum_{i=1}^N \\hat{\\alpha}_i y_i \\langle h(x), h(x_i) \\rangle + \\hat{\\beta_0} \\right) = \\text{sign} \\left( \\sum_{i=1}^N \\hat{\\alpha}_i y_i K(x,x_i) + \\hat{\\beta_0} \\right) \\end{align*} \\] Model fit if (REDO_MODEL_FITTING) { ### control the computational nuances of the train function #################### svm.fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, ## Estimate class probabilities classProbs = TRUE, ## Evaluate performance using ## the following function summaryFunction = compute.summary, returnResamp = &quot;all&quot;) ### fit model over different tuning parameters ################################# svm.fit &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;svmPoly&quot;, trControl = svm.fitControl, tuneGrid = expand.grid( degree = c(1, 2, 3, 4), scale = c(0.1, 0.01, 0.001), C = c(0.001, 0.01, 1)), verbose = TRUE, metric = &quot;mcc.mccf1&quot; ) # specify which metric to optimize save(svm.fit, file = &quot;../data/svm.fit.RData&quot;) } else { load(&quot;../data/svm.fit.RData&quot;) } ggplot(svm.fit) + labs( title = &quot;Support Vector Machine with elastic net regularization&quot;, subtitle = &quot;Degree of kernel vs. F1-score&quot;, x = &quot;Degree of kernel&quot;, y = &quot;F1-score (repeated cross-validation)&quot; ) Model information and evaluation on training set svm.bestTune.mccf1 &lt;- svm.fit$resample %&gt;% group_by(degree, scale, C) %&gt;% summarize(mccf1 = mean(mcc.mccf1), .groups = &quot;drop&quot;) %&gt;% slice(which.max(mccf1)) svm.bestTune.roc &lt;- svm.fit$resample %&gt;% group_by(degree, scale, C) %&gt;% summarize(auc = mean(auc.roc), .groups = &quot;drop&quot;) %&gt;% slice(which.max(auc)) # Hyperparameters of best models for MCC-F1 and ROC cutpoints hyperparameters.table &lt;- tibble( cutpoint = c(&quot;MCCF1&quot;, &quot;ROC&quot;), degree = c(svm.bestTune.mccf1$degree, svm.bestTune.roc$degree), scale = c(svm.bestTune.mccf1$scale, svm.bestTune.roc$scale), C = c(svm.bestTune.mccf1$C, svm.bestTune.roc$C) ) pred.probability &lt;- predict(svm.fit, newdata = data.training, type = &quot;prob&quot;) svm.metrics &lt;- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted) ## [1] &quot;Classification threshold 0.1792 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.1877 maximizes MCCF1 metric.&quot; illustrate.metrics.boxplot(svm.fit$resample %&gt;% filter(degree == hyperparameters.table$degree[1] &amp; scale == hyperparameters.table$scale[1] &amp; C == hyperparameters.table$C[1]), svm.fit$resample %&gt;% filter(degree == hyperparameters.table$degree[2] &amp; scale == hyperparameters.table$scale[2] &amp; C == hyperparameters.table$C[2]), &quot;svm&quot;) Performance evaluation on test set # MCCF1 variant: prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) pred.test.probability &lt;- predict(svm.fit, newdata = data.test, type = &quot;prob&quot;) # ROC variant: fit model using best hyperparameters and # find best threshold and # prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) set.seed(1) svm.fit.roc &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;svmPoly&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = compute.summary), tuneGrid = data.frame( degree = svm.bestTune.roc$degree, scale = svm.bestTune.roc$scale, C = svm.bestTune.roc$C ), verbose = TRUE, metric = &quot;auc.roc&quot;) # specify which metric to optimize svm.metrics.roc.cutpoint &lt;- compute.roc( predict(svm.fit.roc, newdata = data.training, type = &quot;prob&quot;)$delisted, data.training$response)$best_cutpoint pred.test.probability.roc &lt;- predict(svm.fit.roc, newdata = data.test, type = &quot;prob&quot;) # metrics on test set using best threshold determined based on train data svm.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted &gt;= svm.metrics.roc.cutpoint), compute.metrics(data.test$response, pred.test.probability$delisted &gt;= svm.metrics$cutpoint.mccf1)) %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(svm.summary.test, caption = &quot;Support Vector Machine on test data&quot;) Support Vector Machine on test data (continued below) splitting tp fn fp tn accuracy precision recall f1 ROC 313 123 532 1250 0.7047 0.3704 0.7179 0.4887 MCCF1 262 174 354 1428 0.7619 0.4253 0.6009 0.4981 specificity mcc 0.7015 0.3432 0.8013 0.3569 Marginal plots # marginal plots of features in com.features.fin on test data for (feature_identifier in com.features.fin$identifier) { plot &lt;- hist_delisting_frequency( (data.test %&gt;% inner_join(com %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))), feature_identifier, feature_identifier, marginal.data = pred.test.probability$delisted ) print(plot) } 3.4 Neural net # function and object definitions for custom model in caret package # https://topepo.github.io/caret/using-your-own-model-in-train.html preProcValues &lt;- preProcess(data.training, method = c(&quot;center&quot;, &quot;scale&quot;)) # adjusted from https://github.com/topepo/caret/blob/master/models/files/mlpKerasDropout.R keras.list &lt;- list(label = &quot;Multilayer Perceptron Network&quot;, method = &quot;keras&quot;, library = &quot;keras&quot;, loop = NULL, type = c(&quot;Classification&quot;, &quot;Regression&quot;), parameters = data.frame( parameter = c(&#39;layer1&#39;, &#39;layer2&#39;, &quot;layer3&quot;, &quot;batch_size&quot;,&quot;activation&quot;, &quot;regularization_factor&quot;, &quot;dropout_rate&quot;), class = c(rep(&#39;numeric&#39;, 4), &quot;character&quot;, rep(&quot;numeric&quot;, 2)), label = c(&quot;#units in hidden layer 1&quot;, &quot;#units in hidden layer 2&quot;, &quot;#units in hidden layer 3&quot;, &quot;batch size&quot;, &quot;activation Function&quot;, &quot;regularization factor&quot;, &quot;dropout rate&quot;) ), grid = function(x, y, len = NULL, search = &quot;grid&quot;) { afuncs &lt;- c(&quot;sigmoid&quot;, &quot;relu&quot;, &quot;tanh&quot;) if(search == &quot;grid&quot;) { out &lt;- expand.grid( layer1 = ((1:len) * 2) - 1, layer2 = ((1:len) * 2) - 1, layer3 = ((1:len) * 2) - 1, batch_size = floor(nrow(x)/3), activation = &quot;relu&quot;, regularization_factor = c(0.01, 0.001), dropout_rate = c(0.05, 0.2) ) } else { n &lt;- nrow(x) out &lt;- data.frame( layer1 = sample(2:20, replace = TRUE, size = len), layer2 = sample(2:20, replace = TRUE, size = len), layer3 = sample(2:20, replace = TRUE, size = len), batch_size = floor(n*runif(len, min = .1)), activation = sample( afuncs, size = len, replace = TRUE ), regularization_factor = 0.01 * 10^(1:len), dropout_rate = seq(0.05, 0.5, length.out = len) ) } out }, fit = function(x, y, wts, param, lev, last, classProbs, ...) { require(dplyr) K &lt;- keras::backend() K$clear_session() if(!is.matrix(x)) x &lt;- as.matrix(x) model &lt;- keras::keras_model_sequential() model %&gt;% keras::layer_dense(name = &quot;DeepLayer1&quot;, units = param$layer1, activation = as.character(param$activation), input_shape = dim(x)[2], kernel_regularizer = regularizer_l2(param$regularization_factor)) %&gt;% keras::layer_dropout( rate = param$dropout_rate ) %&gt;% keras::layer_dense(name = &quot;DeepLayer2&quot;, units = param$layer2, activation = as.character(param$activation), kernel_regularizer = regularizer_l2(param$regularization_factor)) %&gt;% keras::layer_dropout( rate = param$dropout_rate ) %&gt;% keras::layer_dense(name = &quot;DeepLayer3&quot;, units = param$layer3, activation = as.character(param$activation), kernel_regularizer = regularizer_l2(param$regularization_factor)) %&gt;% keras::layer_dropout( rate = param$dropout_rate ) %&gt;% keras::layer_dense(name = &quot;OutputLayer&quot;, units = 1, activation = &quot;sigmoid&quot;) y &lt;- class2ind(y) model %&gt;% keras::compile( loss = &quot;binary_crossentropy&quot;, optimizer = &quot;adam&quot;, # https://arxiv.org/abs/1412.6980v8 metric = &quot;binary_accuracy&quot; ) model %&gt;% keras::fit( x = x, y = y[,2], batch_size = param$batch_size, epochs = 100, ... ) if (last) { model &lt;- keras::serialize_model(model) } list(object = model) }, predict = function(modelFit, newdata, submodels = NULL) { if(inherits(modelFit$object, &quot;raw&quot;)) modelFit$object &lt;- keras::unserialize_model(modelFit$object) if(!is.matrix(newdata)) newdata &lt;- as.matrix(newdata) out &lt;- predict(modelFit$object, newdata) # check for model type if(ncol(out) == 1) { out &lt;- rep(&quot;active&quot;,length(out))#cbind(1-out[, 1], out[,1]) } else { out &lt;- modelFit$obsLevels[apply(out, 1, which.max)] } out }, prob = function(modelFit, newdata, submodels = NULL) { if(inherits(modelFit$object, &quot;raw&quot;)) modelFit$object &lt;- keras::unserialize_model(modelFit$object) if(!is.matrix(newdata)) newdata &lt;- as.matrix(newdata) out &lt;- predict(modelFit$object, newdata) out &lt;- cbind(1-out, out) colnames(out) &lt;- modelFit$obsLevels as.data.frame(out, stringsAsFactors = TRUE) }, varImp = NULL, tags = c(&quot;Neural Network&quot;), sort = function(x) x, notes = paste(&quot;After `train` completes, the keras model object is serialized&quot;, &quot;so that it can be used between R session. When predicting, the&quot;, &quot;code will temporarily unsearalize the object. To make the&quot;, &quot;predictions more efficient, the user might want to use &quot;, &quot;`keras::unsearlize_model(object$finalModel$object)` in the current&quot;, &quot;R session so that that operation is only done once.&quot;, &quot;Also, this model cannot be run in parallel due to&quot;, &quot;the nature of how tensorflow does the computations.&quot;, &quot;Finally, the cost parameter weights the first&quot;, &quot;class in the outcome vector.&quot;, &quot;Unlike other packages used by `train`, the `dplyr`&quot;, &quot;package is fully loaded when this model is used.&quot;), check = function(pkg) { testmod &lt;- try(keras::keras_model_sequential(), silent = TRUE) if(inherits(testmod, &quot;try-error&quot;)) stop(&quot;Could not start a sequential model. &quot;, &quot;`tensorflow` might not be installed. &quot;, &quot;See `?install_tensorflow`.&quot;, call. = FALSE) TRUE }) Model fit if (REDO_MODEL_FITTING) { ### control the computational nuances of the train function #################### keras.fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 6, ## Estimate class probabilities classProbs = TRUE, #preProc = c(&quot;center&quot;, &quot;scaling&quot;), ## Evaluate performance using ## the following function summaryFunction = compute.summary, returnResamp = &quot;all&quot; ) ### fit model over different tuning parameters ################################# set.seed(1) keras.fit &lt;- caret::train(y = data.training$response, x = as.matrix(data.training %&gt;% select(-permno, -public_date, -response)), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = keras.list, trControl = keras.fitControl, tuneGrid = expand.grid( layer1 = c(35, 38), layer2 = c(20, 25), layer3 = c(12, 18), batch_size = c(16), activation = c(&quot;relu&quot;), regularization_factor = c(0), dropout_rate = c(0, 0.1)), metric = &quot;mcc.mccf1&quot; ) # specify which metric to optimize # Source: Dropout https://jmlr.org/papers/v15/srivastava14a.html save(keras.fit, file = &quot;../data/keras.fit.RData&quot;) } else { load(&quot;../data/keras.fit.RData&quot;) } Model information and evaluation on training set keras.bestTune.mccf1 &lt;- keras.fit$resample %&gt;% group_by(layer1, layer2, layer3, batch_size, activation, regularization_factor, dropout_rate) %&gt;% summarize(mccf1 = mean(mcc.mccf1), .groups = &quot;drop&quot;) %&gt;% slice(which.max(mccf1)) keras.bestTune.roc &lt;- keras.fit$resample %&gt;% group_by(layer1, layer2, layer3, batch_size, activation, regularization_factor, dropout_rate) %&gt;% summarize(auc = mean(auc.roc), .groups = &quot;drop&quot;) %&gt;% slice(which.max(auc)) # Hyperparameters of best models for MCC-F1 and ROC cutpoints hyperparameters.table &lt;- tibble( cutpoint = c(&quot;MCCF1&quot;, &quot;ROC&quot;), layer1 = c(keras.bestTune.mccf1$layer1, keras.bestTune.roc$layer1), layer2 = c(keras.bestTune.mccf1$layer2, keras.bestTune.roc$layer2), layer3 = c(keras.bestTune.mccf1$layer3, keras.bestTune.roc$layer3), batch_size = c(keras.bestTune.mccf1$batch_size, keras.bestTune.roc$batch_size), activation = c(keras.bestTune.mccf1$activation, keras.bestTune.roc$activation), regularization_factor = c(keras.bestTune.mccf1$regularization_factor, keras.bestTune.roc$regularization_factor), dropout_rate = c(keras.bestTune.mccf1$dropout_rate, keras.bestTune.roc$dropout_rate) ) pred.probability &lt;- keras.fit %&gt;% predict(data.training %&gt;% select(-permno, -public_date), type = &quot;prob&quot;) ## 208/208 - 0s - 146ms/epoch - 701us/step keras.metrics &lt;- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted) ## [1] &quot;Classification threshold 0.1862 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.2751 maximizes MCCF1 metric.&quot; illustrate.metrics.boxplot(keras.fit$resample %&gt;% filter(layer1 == hyperparameters.table$layer1[1] &amp; layer2 == hyperparameters.table$layer2[1] &amp; layer3 == hyperparameters.table$layer3[1] &amp; batch_size == hyperparameters.table$batch_size[1] &amp; activation == hyperparameters.table$activation[1] &amp; regularization_factor == hyperparameters.table$regularization_factor[1] &amp; dropout_rate == hyperparameters.table$dropout_rate[1]), keras.fit$resample %&gt;% filter(layer1 == hyperparameters.table$layer1[2] &amp; layer2 == hyperparameters.table$layer2[2] &amp; layer3 == hyperparameters.table$layer3[2] &amp; batch_size == hyperparameters.table$batch_size[2] &amp; activation == hyperparameters.table$activation[2] &amp; regularization_factor == hyperparameters.table$regularization_factor[2] &amp; dropout_rate == hyperparameters.table$dropout_rate[2]), &quot;keras&quot;) Performance evaluation on test set # MCCF1 variant: prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) pred.test.probability &lt;- predict(keras.fit, newdata = data.test, type = &quot;prob&quot;) ## 70/70 - 0s - 79ms/epoch - 1ms/step # ROC variant: fit model using best hyperparameters and # find best threshold and # prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) if (REDO_MODEL_FITTING) { set.seed(1) keras.fit.roc &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = keras.list, trControl = trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = compute.summary), tuneGrid = data.frame( layer1 = keras.bestTune.roc$layer1, layer2 = keras.bestTune.roc$layer2, layer3 = keras.bestTune.roc$layer3, batch_size = keras.bestTune.roc$batch_size, activation = keras.bestTune.roc$activation, regularization_factor = keras.bestTune.roc$regularization_factor, dropout_rate = keras.bestTune.roc$dropout_rate ), verbose = TRUE, metric = &quot;auc.roc&quot;) # specify which metric to optimize save(keras.fit.roc, file = &quot;../data/keras.fit.roc.RData&quot;) } else { load(&quot;../data/keras.fit.roc.RData&quot;) } keras.metrics.roc.cutpoint &lt;- compute.roc( predict(keras.fit.roc, newdata = data.training, type = &quot;prob&quot;)$delisted, data.training$response)$best_cutpoint ## 208/208 - 0s - 152ms/epoch - 731us/step pred.test.probability.roc &lt;- predict(keras.fit.roc, newdata = data.test, type = &quot;prob&quot;) ## 70/70 - 0s - 69ms/epoch - 981us/step # metrics on test set using best threshold determined based on train data keras.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted &gt;= keras.metrics$cutpoint.roc), compute.metrics(data.test$response, pred.test.probability$delisted &gt;= keras.metrics$cutpoint.mccf1)) %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(keras.summary.test, caption = &quot;Neural net on test data&quot;) Neural net on test data (continued below) splitting tp fn fp tn accuracy precision recall f1 ROC 288 148 546 1236 0.6871 0.3453 0.6606 0.4535 MCCF1 239 197 334 1448 0.7606 0.4171 0.5482 0.4737 specificity mcc 0.6936 0.2906 0.8126 0.3275 Marginal plots # marginal plots of features in com.features.fin on test data for (feature_identifier in com.features.fin$identifier) { plot &lt;- hist_delisting_frequency( (data.test %&gt;% inner_join(com %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))), feature_identifier, feature_identifier, marginal.data = pred.test.probability$delisted ) print(plot) } 3.5 Boosting https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf Hyperparameters n.trees: number of iterations \\(T\\) interaction.depth: depth of each tree \\(K\\) shrinkage: shrinkage (or learning rate) parameter \\(\\lambda\\) ```bag.fraction````: subsampling rate \\(p\\) Initialize \\(\\hat{f}(x)\\) to be a constant and compute \\(\\hat{f}(x) = \\arg \\min_\\rho L(y_i,\\rho)\\) For \\(t\\) in \\(1,\\dots,T\\) do Compute the negative gradient as the working response \\[ z_i = - \\left. \\frac{\\partial}{\\partial f(x_i)} L(y_i, f(x_i)) \\right|_{f(x_i)=\\hat{f}(x_i)} \\qquad \\forall i \\in \\{1,\\dots,N\\} \\] Randomly select \\(p \\cdot N\\) cases from the data set. Fit a regression tree with \\(K\\) terminal nodes, \\(g(x) = E(z|x)\\). This tree is fit using only those randomly selected observations from step 2. Compute the optimal terminal node predictions, \\(\\rho_1,\\dots,\\rho_K\\), as \\[ \\rho_k = \\arg \\min_\\rho \\sum_{x_i \\in S_k}^{N} L(y_i, \\hat{f}(x_i) + \\rho) \\] where \\(S_k\\) is the set of \\(x\\)s that define terminal node \\(k\\). This step uses only the randomly selected observations. Update the \\(\\hat{f}(x)\\) as \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\rho_{k(x)}\\) where \\(k(x)\\) indicates the index of the terminal node into which an observation with features \\(x\\) would fall. Model fit if (REDO_MODEL_FITTING) { ### control the computational nuances of the train function #################### boosting.fitControl &lt;- trainControl(method = &quot;cv&quot;, number = 10, ## Estimate class probabilities classProbs = TRUE, ## Evaluate performance using ## the following function summaryFunction = compute.summary, returnResamp = &quot;all&quot;) ### fit model over different tuning parameters ################################# set.seed(1) boosting.fit &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date) %&gt;% mutate(response = as.factor(response)), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;gbm&quot;, trControl = boosting.fitControl, tuneGrid = expand.grid( n.trees = c(100, 150, 250, 500), interaction.depth = c(3, 5, 8), # Hands-On Machine Learning with R: https://bradleyboehmke.github.io/HOML/ shrinkage = c(0.001, 0.01, 0.1, 0.2), n.minobsinnode = c(5, 10)), # bag.fraction = 0.5 by default verbose = FALSE, metric = &quot;mcc.mccf1&quot; ) # specify which metric to optimize save(boosting.fit, file = &quot;../data/boosting.fit.RData&quot;) } else { load(&quot;../data/boosting.fit.RData&quot;) } # effect of hyperparameters ggplot(boosting.fit) + labs( title = &quot;Gradient boosting with regression trees as base-learners&quot;, subtitle = &quot;MCC vs. min. terminal node size &amp; shrinkage parameter alpha&quot;, x = &quot;Min. terminal node size &amp; shrinkage parameter&quot;, y = &quot;MCC (10-fold cross-validation)&quot; ) Model information and evaluation on training set boosting.bestTune.mccf1 &lt;- boosting.fit$resample %&gt;% group_by(n.trees, interaction.depth, shrinkage, n.minobsinnode) %&gt;% summarize(mccf1 = mean(mcc.mccf1), .groups = &quot;drop&quot;) %&gt;% slice(which.max(mccf1)) boosting.bestTune.roc &lt;- boosting.fit$resample %&gt;% group_by(n.trees, interaction.depth, shrinkage, n.minobsinnode) %&gt;% summarize(auc = mean(auc.roc), .groups = &quot;drop&quot;) %&gt;% slice(which.max(auc)) # Hyperparameters of best models for MCC-F1 and ROC cutpoints hyperparameters.table &lt;- tibble( cutpoint = c(&quot;MCCF1&quot;, &quot;ROC&quot;), n.trees = c(boosting.bestTune.mccf1$n.trees, boosting.bestTune.roc$n.trees), interaction.depth = c(boosting.bestTune.mccf1$interaction.depth, boosting.bestTune.roc$interaction.depth), shrinkage = c(boosting.bestTune.mccf1$shrinkage, boosting.bestTune.roc$shrinkage), n.minobsinnode = c(boosting.bestTune.mccf1$n.minobsinnode, boosting.bestTune.roc$n.minobsinnode) ) pred.probability &lt;- predict(boosting.fit, newdata = data.training, type = &quot;prob&quot;) boosting.metrics &lt;- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted) ## [1] &quot;Classification threshold 0.2046 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.2304 maximizes MCCF1 metric.&quot; illustrate.metrics.boxplot(boosting.fit$resample %&gt;% filter(n.trees == hyperparameters.table$n.trees[1] &amp; interaction.depth == hyperparameters.table$interaction.depth[1] &amp; shrinkage == hyperparameters.table$shrinkage[1] &amp; n.minobsinnode == hyperparameters.table$n.minobsinnode[1]), boosting.fit$resample %&gt;% filter(n.trees == hyperparameters.table$n.trees[2] &amp; interaction.depth == hyperparameters.table$interaction.depth[2] &amp; shrinkage == hyperparameters.table$shrinkage[2] &amp; n.minobsinnode == hyperparameters.table$n.minobsinnode[2]), &quot;boosting&quot;) Performance evaluation on test set # MCCF1 variant: prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) pred.test.probability &lt;- predict(boosting.fit, newdata = data.test, type = &quot;prob&quot;) # ROC variant: fit model using best hyperparameters and # find best threshold and # prediction scores on test data (&quot;active&quot;, &quot;delisted&quot;) set.seed(1) boosting.fit.roc &lt;- train(response ~ ., data = data.training %&gt;% select(-permno, -public_date), preProcess = c(&quot;center&quot;, &quot;scale&quot;), method = &quot;gbm&quot;, trControl = trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = compute.summary), tuneGrid = expand.grid( n.trees = boosting.bestTune.roc$n.trees, interaction.depth = boosting.bestTune.roc$interaction.depth, shrinkage = boosting.bestTune.roc$shrinkage, n.minobsinnode = boosting.bestTune.roc$n.minobsinnode), metric = &quot;auc.roc&quot;) # specify which metric to optimize boosting.metrics.roc.cutpoint &lt;- compute.roc( predict(boosting.fit.roc, newdata = data.training, type = &quot;prob&quot;)$delisted, data.training$response)$best_cutpoint pred.test.probability &lt;- predict(boosting.fit, newdata = data.test, type = &quot;prob&quot;) # metrics on test set using best threshold determined based on train data boosting.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted &gt;= boosting.metrics.roc.cutpoint), compute.metrics(data.test$response, pred.test.probability$delisted &gt;= boosting.metrics$cutpoint.mccf1)) %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(boosting.summary.test, caption = &quot;Gradient boosting on test data&quot;) Variable importance # returns the reduction attributable to each variable in sum of squared error # in predicting the gradient on each iteration. It describes the relative # influence of each variable in reducing the loss function boosting.vimp.results &lt;- varImp(boosting.fit)$importance boosting.vimp &lt;- boosting.vimp.results %&gt;% mutate(feature = rownames(boosting.vimp.results), vimp = Overall) %&gt;% arrange(desc(vimp)) plot.vimp(boosting.vimp, &quot;Importance based on sum of SSE reductions&quot;) Marginal plots # marginal plots of 20 most important features on test data for (feature_identifier in (boosting.vimp %&gt;% filter(row_number() &lt;= 20))$feature) { plot &lt;- hist_delisting_frequency( (data.test %&gt;% inner_join(com %&gt;% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))), feature_identifier, feature_identifier, marginal.data = pred.test.probability$delisted ) print(plot) } 3.6 Ensemble ensemble_variants &lt;- list(mccf1 = NULL, roc = NULL) if (REDO_MODEL_FITTING) { set.seed(1) my_control &lt;- trainControl( method = &quot;boot&quot;, number = 10, savePredictions = &quot;final&quot;, returnResamp = &quot;final&quot;, classProbs = TRUE, index = createResample(data.training$response, times = 2), summaryFunction = compute.summary ) for (i in 1:length(ensemble_variants)) { if (i &lt;= 1) { metric &lt;- &quot;mcc.mccf1&quot; rf.bestTune &lt;- rf.bestTune.mccf1 lr.bestTune &lt;- lr.bestTune.mccf1 svm.bestTune &lt;- svm.bestTune.mccf1 keras.bestTune &lt;- keras.bestTune.mccf1 boosting.bestTune &lt;- boosting.bestTune.mccf1 } else { metric &lt;- &quot;auc.roc&quot; rf.bestTune &lt;- rf.bestTune.roc lr.bestTune &lt;- lr.bestTune.roc svm.bestTune &lt;- svm.bestTune.roc keras.bestTune &lt;- keras.bestTune.roc boosting.bestTune &lt;- boosting.bestTune.roc } model_list &lt;- caretList( response ~ ., data = data.training %&gt;% select(-permno, -public_date), trControl = my_control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), metric = metric, # either &quot;mcc.roc&quot; or &quot;mcc.mccf1&quot; tuneList=list( rfr = caretModelSpec(method = rf.list, tuneGrid = data.frame(ntree = rf.bestTune$ntree, nodesize = rf.bestTune$nodesize, mtry = rf.bestTune$mtry)), lr = caretModelSpec(method = &quot;glmnet&quot;, tuneGrid = data.frame(alpha = lr.bestTune$alpha, lambda = lr.bestTune$lambda), trace=FALSE), svm = caretModelSpec(method = &quot;svmPoly&quot;, tuneGrid = data.frame( degree = svm.bestTune$degree, scale = svm.bestTune$scale, C = svm.bestTune$C), trace=FALSE), keras = caretModelSpec(method = keras.list, tuneGrid = data.frame( layer1 = keras.bestTune$layer1, layer2 = keras.bestTune$layer2, layer3 = keras.bestTune$layer3, batch_size = keras.bestTune$batch_size, activation = keras.bestTune$activation, regularization_factor = keras.bestTune$regularization_factor, dropout_rate = keras.bestTune$dropout_rate), trace=FALSE), boosting = caretModelSpec(method = &quot;gbm&quot;, tuneGrid = data.frame( n.trees = boosting.bestTune$n.trees, interaction.depth = boosting.bestTune$interaction.depth, shrinkage = boosting.bestTune$shrinkage, n.minobsinnode = boosting.bestTune$n.minobsinnode)) ) ) if (i &lt;= 1) { ensemble_variants$mccf1 &lt;- model_list } else { ensemble_variants$roc &lt;- model_list } } save(ensemble_variants, file = &quot;../data/ensemble_variants_no_gap.RData&quot;) } else { load(&quot;../data/ensemble_variants.RData&quot;) } ensemble.summary.test &lt;- tibble() for (i in 1:length(ensemble_variants)) { if (i &lt;= 1) { # MCCF1 threshold model_list &lt;- ensemble_variants$mccf1 metric &lt;- &quot;mcc.mccf1&quot; } else { # ROC threshold model_list &lt;- ensemble_variants$roc metric &lt;- &quot;auc.roc&quot; } set.seed(1) ensemble &lt;- caretStack(model_list, method = &quot;glm&quot;, metric = metric, # tuneGrid = expand.grid( # alpha = 0, # only ridge (L2) regularization # lambda = c(0, 0.0001, 0.001, 0.01, 0.1) # ), trControl = trainControl( method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = compute.summary ), ) #ensemble$ens_model$bestTune #coef(ensemble$ens_model$finalModel, ensemble$ens_model$bestTune$lambda) print(summary(ensemble$ens_model$finalModel)) model_preds &lt;- lapply(model_list, predict, newdata=data.training, type=&quot;prob&quot;) model_preds &lt;- lapply(model_preds, function(x) x[, &quot;delisted&quot;]) model_preds &lt;- data.frame(model_preds) model_preds$ensemble &lt;- 1-predict(ensemble, newdata = data.training, type = &quot;prob&quot;) print(cor(model_preds)) pander(cor(model_preds), caption = &quot;Correlation of prediction scores&quot;) print(illustrate.metrics(tibble(ensemble=&quot;ensemble&quot;), data.training$response, model_preds$ensemble)) pred.test &lt;- 1-predict(ensemble, newdata = data.test, type=&quot;prob&quot;) model_identifiers &lt;- c(&quot;ensemble&quot;, &quot;gbm&quot;, &quot;glmnet&quot;, &quot;keras&quot;, &quot;kernlab&quot;, &quot;randomForestSRC&quot;) model_labels &lt;- c(&quot;Ensemble&quot;, &quot;Gradient boosting&quot;, &quot;Logistic regression&quot;, &quot;Neural net&quot;, &quot;SVM&quot;, &quot;Random Forest&quot;) if (i &lt;= 1) { # MCCF1 threshold ensemble.cutpoint.mccf1 &lt;- compute.mccf1cutpoint(model_preds$ensemble, data.training$response)$best_cutpoint ensemble.summary.test &lt;- rbind(ensemble.summary.test, compute.metrics(data.test$response, pred.test &gt;= ensemble.cutpoint.mccf1)) # MCCF1 curve plot.mccf1 &lt;- ggplot(mapping = aes(x = f1, y = mcc.nor, col=col)) + labs( title = &quot;MCC-F1 curve&quot;, subtitle = &quot;Base models and ensemble model&quot;, x = &quot;F1 score&quot;, y = &quot;unit-normalized MCC&quot; ) for (model in ensemble$models) { preds &lt;- predict(model, newdata = data.test, type = &quot;prob&quot;)$delisted mccf1.result &lt;- mccf1(preds, data.test$response == &quot;delisted&quot;) plot.mccf1 &lt;- plot.mccf1 + geom_path(data = mccf1.result %&gt;% mutate(col = model$modelInfo$library[1], linewidth = 1)) } plot.mccf1 &lt;- plot.mccf1 + geom_path(data = mccf1(pred.test, data.test$response == &quot;delisted&quot;) %&gt;% mutate(col = &quot;ensemble&quot;), linewidth = 1) + scale_color_discrete(&quot;&quot;, breaks = model_identifiers, labels = model_labels) } else { # ROC threshold ensemble.cutpoint.roc &lt;- compute.roc(model_preds$ensemble, data.training$response)$best_cutpoint ensemble.summary.test &lt;- rbind(compute.metrics(data.test$response, pred.test &gt;= ensemble.cutpoint.roc), ensemble.summary.test) # ROC curve plot.roc &lt;- ggplot(mapping = aes(x = CumulativeFPR, y = CumulativeTPR, col=col)) + labs( title = &quot;ROC curve&quot;, subtitle = &quot;Base models and ensemble model&quot;, x = &quot;False positive rate (1 - specificity)&quot;, y = &quot;True positive rate&quot; ) for (model in ensemble$models) { auc_roc.result &lt;- auc_roc( # plot data for ROC curve predict(model, newdata = data.test, type = &quot;prob&quot;)$delisted, data.test$response == &quot;delisted&quot;, returnDT = TRUE) preds &lt;- predict(model, newdata = data.test, type = &quot;prob&quot;)$delisted plot.roc &lt;- plot.roc + geom_line(data = as_tibble(auc_roc.result) %&gt;% mutate(col = model$modelInfo$library[1], linewidth = 1)) } plot.roc &lt;- plot.roc + geom_line(data = as_tibble( auc_roc(pred.test, data.test$response == &quot;delisted&quot;, returnDT = TRUE)) %&gt;% mutate(col = &quot;ensemble&quot;), linewidth = 1) + coord_fixed() + scale_color_discrete(&quot;&quot;, breaks = model_identifiers, labels = model_labels) } } ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2590 -0.5686 -0.4048 -0.3226 2.5184 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.7275 0.2456 11.104 &lt; 2e-16 *** ## rfr -5.6041 0.6151 -9.111 &lt; 2e-16 *** ## lr 1.7449 0.4582 3.808 0.000140 *** ## svm 0.1236 0.2730 0.453 0.650762 ## keras -0.0266 0.1725 -0.154 0.877474 ## boosting -1.8881 0.5547 -3.404 0.000664 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4786.3 on 4888 degrees of freedom ## Residual deviance: 3887.6 on 4883 degrees of freedom ## AIC: 3899.6 ## ## Number of Fisher Scoring iterations: 5 ## ## 208/208 - 0s - 153ms/epoch - 734us/step ## 208/208 - 0s - 208ms/epoch - 998us/step ## rfr lr svm keras boosting ensemble ## rfr 1.0000000 0.6655141 0.5800887 0.6836591 0.9219664 0.9603817 ## lr 0.6655141 1.0000000 0.5307103 0.5108240 0.6829585 0.5934105 ## svm 0.5800887 0.5307103 1.0000000 0.5305829 0.5914585 0.5829582 ## keras 0.6836591 0.5108240 0.5305829 1.0000000 0.6386746 0.6554106 ## boosting 0.9219664 0.6829585 0.5914585 0.6386746 1.0000000 0.9506085 ## ensemble 0.9603817 0.5934105 0.5829582 0.6554106 0.9506085 1.0000000 ## [1] &quot;Classification threshold 0.1865 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.2179 maximizes MCCF1 metric.&quot; ## $cutpoint.roc ## [1] 0.1864974 ## ## $cutpoint.mccf1 ## [1] 0.217907 ## ## 70/70 - 0s - 93ms/epoch - 1ms/step ## 70/70 - 0s - 79ms/epoch - 1ms/step ## ## Call: ## NULL ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2888 -0.5692 -0.4064 -0.3256 2.4980 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.8708 0.2286 12.556 &lt; 2e-16 *** ## rfr -5.3360 0.6126 -8.711 &lt; 2e-16 *** ## lr 1.3777 0.4050 3.402 0.000669 *** ## svm 0.2306 0.2623 0.879 0.379277 ## keras -0.2404 0.1646 -1.460 0.144213 ## boosting -1.8508 0.5567 -3.324 0.000886 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4786.3 on 4888 degrees of freedom ## Residual deviance: 3894.9 on 4883 degrees of freedom ## AIC: 3906.9 ## ## Number of Fisher Scoring iterations: 5 ## ## 208/208 - 0s - 156ms/epoch - 750us/step ## 208/208 - 0s - 204ms/epoch - 980us/step ## rfr lr svm keras boosting ensemble ## rfr 1.0000000 0.6648830 0.5347826 0.7245870 0.9237242 0.9624202 ## lr 0.6648830 1.0000000 0.4710980 0.5982605 0.6934042 0.6129214 ## svm 0.5347826 0.4710980 1.0000000 0.5230359 0.5474162 0.5418012 ## keras 0.7245870 0.5982605 0.5230359 1.0000000 0.6959119 0.7189632 ## boosting 0.9237242 0.6934042 0.5474162 0.6959119 1.0000000 0.9522992 ## ensemble 0.9624202 0.6129214 0.5418012 0.7189632 0.9522992 1.0000000 ## [1] &quot;Classification threshold 0.1675 minimizes distance of ROC curve to (0,1).&quot; ## [1] &quot;Classification threshold 0.3036 maximizes MCCF1 metric.&quot; ## $cutpoint.roc ## [1] 0.1674565 ## ## $cutpoint.mccf1 ## [1] 0.3035905 ## ## 70/70 - 0s - 78ms/epoch - 1ms/step ## 70/70 - 0s - 83ms/epoch - 1ms/step ## 70/70 - 0s - 81ms/epoch - 1ms/step # metrics on test set using best threshold determined based on train data ensemble.summary.test &lt;- ensemble.summary.test %&gt;% cbind(splitting = c(&quot;ROC&quot;, &quot;MCCF1&quot;), .) pander(ensemble.summary.test, caption = &quot;Ensemble on test data&quot;) Ensemble on test data (continued below) splitting tp fn fp tn accuracy precision recall f1 ROC 338 98 443 1339 0.7561 0.4328 0.7752 0.5555 MCCF1 293 143 328 1454 0.7876 0.4718 0.672 0.5544 specificity mcc 0.7514 0.4382 0.8159 0.4319 ggarrange(plot.roc, plot.mccf1, nrow = 1,ncol = 2, legend = &quot;bottom&quot;, common.legend = TRUE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
