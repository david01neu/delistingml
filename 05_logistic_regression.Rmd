## Logistic Regression

$$
\underset{(\beta_0,\beta)\in R^{(p+1)}}{\min} -\left[ \frac{1}{N} \sum_{i=1}^N y_i \cdot (\beta_0 + x_i^T\beta) - \log(1 + e^{\beta_0+x_i^T\beta}) \right] + \lambda \left[\frac{1-\alpha}{2} \lVert \beta \rVert_2 + \alpha \lVert \beta \rVert_1 \right]
$$

**Model fit**
```{r, model.lr.fit}
if (REDO_MODEL_FITTING) {
  ### control the computational nuances of the train function ##################
  lr.fitControl <- trainControl(method = "cv",
                             number = 10,
                             classProbs = TRUE, # estimate class probabilities
                             # evaluate performance using the following function
                             summaryFunction = compute.summary,
                             returnResamp = "all")
  ### fit model over different tuning parameters ###############################
  lr.fit <- train(response ~ ., data = data.training %>% select(-permno, -public_date),
                   preProcess = c("center", "scale"),
                   method = "glmnet", 
                   trControl = lr.fitControl, 
                   tuneGrid = expand.grid(
                     alpha = seq(0, 1, by = 0.1),
                     lambda = 10^(-seq(1, 6, by = 1))
                   ),
                   verbose = TRUE, 
                   metric = "mcc.mccf1") # specify which metric to optimize
  save(lr.fit, file = "../data/lr.fit.RData")
} else {
  load("../data/lr.fit.RData")
}

ggplot(lr.fit) +
  labs(
    title = "Logistic regression with elastic net regularization",
    subtitle = "MCC vs. mixing percentage alpha",
    x = "Mixing percentage alpha",
    y = "MCC (10-fold cross-validation)"
  )
```

**Model information and evaluation on training set**
```{r, model.lr.best_model}
lr.bestTune.mccf1 <- lr.fit$resample %>%
  group_by(alpha, lambda) %>%
  summarize(mccf1 = mean(mcc.mccf1), .groups = "drop") %>%
  slice(which.max(mccf1))

lr.bestTune.roc <- lr.fit$resample %>%
  group_by(alpha, lambda) %>%
  summarize(auc = mean(auc.roc), .groups = "drop") %>%
  slice(which.max(auc))

# Hyperparameters of best models for MCC-F1 and ROC cutpoints
hyperparameters.table <- tibble(
  cutpoint = c("MCCF1", "ROC"),
  alpha = c(lr.bestTune.mccf1$alpha, lr.bestTune.roc$alpha),
  lambda = c(lr.bestTune.mccf1$lambda, lr.bestTune.roc$lambda),
)

pred.probability <- predict(lr.fit, newdata = data.training, type = "prob")
lr.metrics <- illustrate.metrics(hyperparameters.table, data.test$response, pred.test.probability$delisted)

illustrate.metrics.boxplot(lr.fit$resample %>% filter(alpha == hyperparameters.table$alpha[1]
                                                      & lambda == hyperparameters.table$lambda[1]),
                           lr.fit$resample %>% filter(alpha == hyperparameters.table$alpha[2]
                                                      & lambda == hyperparameters.table$lambda[2]),
                           "lr")
```
**Performance evaluation on test set**
```{r, lr.test}
# MCCF1 variant: prediction scores on test data ("active", "delisted")
pred.test.probability <- predict(lr.fit, newdata = data.test, type = "prob")

# ROC variant: fit model using best hyperparameters and 
#              find best threshold and
#              prediction scores on test data ("active", "delisted")
set.seed(1)
lr.fit.roc <- train(response ~ ., data = data.training %>% select(-permno, -public_date),
                   preProcess = c("center", "scale"),
                   method = "glmnet", 
                   trControl = trainControl(method = "cv", number = 10,
                             classProbs = TRUE, summaryFunction = compute.summary), 
                   tuneGrid = data.frame(
                     alpha = lr.bestTune.roc$alpha,
                     lambda = lr.bestTune.roc$lambda
                   ),
                   verbose = TRUE, 
                   metric = "auc.roc") # specify which metric to optimize
lr.metrics.roc.cutpoint <- compute.roc(
  predict(lr.fit.roc, newdata = data.training, type = "prob")$delisted,
  data.training$response)$best_cutpoint
pred.test.probability.roc <- predict(lr.fit.roc, newdata = data.test, type = "prob")

# metrics on test set using best threshold determined based on train data
lr.summary.test <- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted >= lr.metrics.roc.cutpoint),
      compute.metrics(data.test$response, pred.test.probability$delisted >= lr.metrics$cutpoint.mccf1)) %>%
  cbind(splitting = c("ROC", "MCCF1"), .)
pander(lr.summary.test, caption = "Logistic regression on test data")
```

**Variable importance**
```{r, model.lr.vimp}
lr.vimp.results <- varImp(lr.fit)$importance
lr.vimp <- lr.vimp.results %>%
  mutate(feature = rownames(lr.vimp.results),
         vimp = Overall) %>% 
  arrange(desc(vimp))

plot.vimp(lr.vimp, "Importance based on value of t-statistic for each model parameter")
```

**Marginal plots**
```{r, model.lr.marginal}
# marginal plots of 20 most important features and feature pe_exi on test data
for (feature_identifier in c((lr.vimp %>% filter(row_number() <= 20))$feature, "pe_exi")) {
  plot <- hist_delisting_frequency(
      (data.test %>% 
        inner_join(com %>% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))),
      feature_identifier,
      feature_identifier,
      marginal.data = pred.test.probability$delisted
  )
  print(plot)
}
```
