## Ensemble
```{r, model.ensemble.init}
ensemble_variants <- list(mccf1 = NULL, roc = NULL)
```

```{r, model.ensemble.fit}
if (REDO_MODEL_FITTING) {
  set.seed(1)
  my_control <- trainControl(
    method = "boot",
    number = 10,
    savePredictions = "final",
    returnResamp = "final",
    classProbs = TRUE,
    index = createResample(data.training$response, times = 2),
    summaryFunction = compute.summary
  )

  for (i in 1:length(ensemble_variants)) {
    if (i <= 1) {
      metric <- "mcc.mccf1"
      rf.bestTune <- rf.bestTune.mccf1
      lr.bestTune <- lr.bestTune.mccf1
      svm.bestTune <- svm.bestTune.mccf1
      keras.bestTune <- keras.bestTune.mccf1
      boosting.bestTune <- boosting.bestTune.mccf1
    } else {
      metric <- "auc.roc"
      rf.bestTune <- rf.bestTune.roc
      lr.bestTune <- lr.bestTune.roc
      svm.bestTune <- svm.bestTune.roc
      keras.bestTune <- keras.bestTune.roc
      boosting.bestTune <- boosting.bestTune.roc
    }
    
    model_list <- caretList(
      response ~ .,
      data = data.training %>% select(-permno, -public_date),
      trControl = my_control,
      preProcess = c("center", "scale"),
      metric = metric, # either "mcc.roc" or "mcc.mccf1"
      tuneList=list(
        rfr = caretModelSpec(method = rf.list,
                             tuneGrid = data.frame(ntree = rf.bestTune$ntree,
                                                   nodesize = rf.bestTune$nodesize,
                                                   mtry = rf.bestTune$mtry)),
        lr = caretModelSpec(method = "glmnet",
                            tuneGrid = data.frame(alpha = lr.bestTune$alpha, lambda = lr.bestTune$lambda),
                            trace=FALSE),
        svm = caretModelSpec(method = "svmPoly",
                            tuneGrid = data.frame(
                               degree = svm.bestTune$degree,
                               scale = svm.bestTune$scale,
                               C = svm.bestTune$C),
                            trace=FALSE),
        keras = caretModelSpec(method = keras.list,
                            tuneGrid = data.frame(
                               layer1 = keras.bestTune$layer1,
                               layer2 = keras.bestTune$layer2,
                               layer3 = keras.bestTune$layer3,
                               batch_size = keras.bestTune$batch_size,
                               activation = keras.bestTune$activation,
                               regularization_factor = keras.bestTune$regularization_factor,
                               dropout_rate = keras.bestTune$dropout_rate),
                            trace=FALSE),
        boosting = caretModelSpec(method = "gbm",
                        tuneGrid = data.frame(
                          n.trees = boosting.bestTune$n.trees,
                          interaction.depth = boosting.bestTune$interaction.depth,
                          shrinkage = boosting.bestTune$shrinkage,
                          n.minobsinnode = boosting.bestTune$n.minobsinnode))
      )
    )
    
    if (i <= 1) {
      ensemble_variants$mccf1 <- model_list
    } else {
      ensemble_variants$roc <- model_list
    }
  }
  save(ensemble_variants, file = "../data/ensemble_variants_no_gap.RData")
} else {
  load("../data/ensemble_variants.RData")
}
```

```{r, model.ensemble.test}
ensemble.summary.test <- tibble()
for (i in 1:length(ensemble_variants)) {
  if (i <= 1) { # MCCF1 threshold
    model_list <- ensemble_variants$mccf1
    metric <- "mcc.mccf1"
  } else { # ROC threshold
    model_list <- ensemble_variants$roc
    metric <- "auc.roc"
  }
  
  set.seed(1)
  ensemble <- caretStack(model_list, method = "glm", metric = metric,
              # tuneGrid = expand.grid(
              #   alpha = 0, # only ridge (L2) regularization
              #   lambda = c(0, 0.0001, 0.001, 0.01, 0.1)
              # ),
              trControl = trainControl(
               method = "cv", number = 10,
               classProbs = TRUE,
               summaryFunction = compute.summary
               ),
              )

  print(summary(ensemble$ens_model$finalModel))
  print(confint(ensemble$ens_model$finalModel))
  
  model_preds <- lapply(model_list, predict, newdata=data.training, type="prob")
  model_preds <- lapply(model_preds, function(x) x[, "delisted"])
  model_preds <- data.frame(model_preds)
  
  model_preds$ensemble <- 1-predict(ensemble, newdata = data.training, type = "prob")
  print(cor(model_preds))
  pander(cor(model_preds), caption = "Correlation of prediction scores")
  print(illustrate.metrics(tibble(ensemble="ensemble"), data.training$response, model_preds$ensemble))
  
  pred.test <- 1-predict(ensemble, newdata = data.test, type="prob")
  
  model_identifiers <- c("ensemble", "randomForestSRC", "gbm", "keras", "kernlab", "glmnet")
  model_labels <- c("Ensemble", "Random Forest", "Gradient boosting", "Neural net", "SVM", "Logistic regression")

  if (i <= 1) { # MCCF1 threshold
    ensemble.cutpoint.mccf1 <- compute.mccf1cutpoint(model_preds$ensemble, data.training$response)$best_cutpoint
    ensemble.summary.test <- rbind(ensemble.summary.test, compute.metrics(data.test$response, pred.test >= ensemble.cutpoint.mccf1))
    # MCCF1 curve
    plot.mccf1 <- ggplot(mapping = aes(x = f1, y = mcc.nor, col=col)) +
            labs(
              title = "MCC-F1 curve",
              subtitle = "Base models and ensemble model",
              x = "F1 score",
              y = "unit-normalized MCC"
            )

    for (model in ensemble$models) {
      preds <- predict(model, newdata = data.test, type = "prob")$delisted
      mccf1.result <- mccf1(preds, data.test$response == "delisted")
      plot.mccf1 <- plot.mccf1 +
            geom_path(data = mccf1.result %>% 
                        mutate(col = model$modelInfo$library[1], linewidth = 1))
    }

    plot.mccf1 <- plot.mccf1 + geom_path(data = mccf1(pred.test, data.test$response == "delisted") %>% 
                    mutate(col = "ensemble"), linewidth = 1) + 
                  scale_color_discrete("", breaks = model_identifiers, labels = model_labels)
  } else { # ROC threshold
    ensemble.cutpoint.roc <- compute.roc(model_preds$ensemble, data.training$response)$best_cutpoint
    ensemble.summary.test <- rbind(compute.metrics(data.test$response, pred.test >= ensemble.cutpoint.roc),
                                   ensemble.summary.test)
    # ROC curve
    plot.roc <- ggplot(mapping = aes(x = CumulativeFPR, y = CumulativeTPR, col=col)) +
        labs(
          title = "ROC curve",
          subtitle = "Base models and ensemble model",
          x = "False positive rate (1 - specificity)",
          y = "True positive rate"
        )
    
    for (model in ensemble$models) {
      auc_roc.result <- auc_roc( # plot data for ROC curve
        predict(model, newdata = data.test, type = "prob")$delisted,
        data.test$response == "delisted",
        returnDT = TRUE)
      preds <- predict(model, newdata = data.test, type = "prob")$delisted
      plot.roc <- plot.roc +
            geom_line(data = as_tibble(auc_roc.result) %>% 
                        mutate(col = model$modelInfo$library[1], linewidth = 1))
    }
    plot.roc <- plot.roc + geom_line(data = as_tibble(
                auc_roc(pred.test, data.test$response == "delisted", returnDT = TRUE)) %>%
              mutate(col = "ensemble"), linewidth = 1) +
              coord_fixed() +
              scale_color_discrete("", breaks = model_identifiers, labels = model_labels)
  }
}

# metrics on test set using best threshold determined based on train data
ensemble.summary.test <- ensemble.summary.test %>%
  cbind(splitting = c("ROC", "MCCF1"), .) 
pander(ensemble.summary.test, caption = "Ensemble on test data")

ggarrange(plot.roc, plot.mccf1, nrow = 1, ncol = 2,
          legend = "bottom", common.legend = TRUE)
```