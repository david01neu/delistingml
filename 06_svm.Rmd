## Support Vector Machine
<!-- SVM vignette: https://cran.r-project.org/web/packages/kernlab/vignettes/kernlab.pdf -->
Let the training data consist of $N$ pairs $(x_1,y_1),(x_2,y_2),\ldots,(x_N,y_N)$, with $x_i \in \mathbb{R}^p$ and $y_i\in\{0,1\}$. Define a hyperplane by
$$
\{x:f(x)=x^T\beta+\beta_0=0\}
$$
where $\beta$ is a vector.
$$
\begin{align*}
  \hat{f}(x) &= \text{sign} \left( h(x)^T \hat{\beta} + \hat{\beta_0} \right) = \text{sign} \left( h(x)^T \left( \sum_{i=1}^N \hat{\alpha_i} y_i h(x_i) \right) + \hat{\beta_0} \right) \\
  	&= \text{sign} \left( \sum_{i=1}^N \hat{\alpha}_i y_i \langle h(x), h(x_i) \rangle + \hat{\beta_0} \right) = \text{sign} \left( \sum_{i=1}^N \hat{\alpha}_i y_i K(x,x_i) + \hat{\beta_0} \right)
\end{align*}
$$

**Model fit**
```{r, model.svm.fit}
if (REDO_MODEL_FITTING) {
  ### control the computational nuances of the train function ####################
  svm.fitControl <- trainControl(method = "cv",
                             number = 10,
                             ## Estimate class probabilities
                             classProbs = TRUE,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = compute.summary,
                             returnResamp = "all")
  
  ### fit model over different tuning parameters #################################
  svm.fit <- train(response ~ ., data = data.training %>% select(-permno, -public_date), 
                   preProcess = c("center", "scale"),
                   method = "svmPoly", 
                   trControl = svm.fitControl,
                   tuneGrid = expand.grid(
                     degree = c(1, 2, 3, 4),
                     scale = c(0.1, 0.01, 0.001),
                     C = c(0.001, 0.01, 1)),
                   verbose = TRUE,
                   metric = "mcc.mccf1"
                   ) # specify which metric to optimize
  save(svm.fit, file = "../data/svm.fit.RData")
} else {
  load("../data/svm.fit.RData")
}

ggplot(svm.fit) +
  labs(
    title = "Support Vector Machine with elastic net regularization",
    subtitle = "Degree of kernel vs. F1-score",
    x = "Degree of kernel",
    y = "F1-score (repeated cross-validation)"
  )
```

**Model information and evaluation on training set**
```{r, model.svm.best_model}
svm.bestTune.mccf1 <- svm.fit$resample %>%
  group_by(degree, scale, C) %>%
  summarize(mccf1 = mean(mcc.mccf1), .groups = "drop") %>%
  slice(which.max(mccf1))

svm.bestTune.roc <- svm.fit$resample %>%
  group_by(degree, scale, C) %>%
  summarize(auc = mean(auc.roc), .groups = "drop") %>%
  slice(which.max(auc))

# Hyperparameters of best models for MCC-F1 and ROC cutpoints
hyperparameters.table <- tibble(
  cutpoint = c("MCCF1", "ROC"),
  degree = c(svm.bestTune.mccf1$degree, svm.bestTune.roc$degree),
  scale = c(svm.bestTune.mccf1$scale, svm.bestTune.roc$scale),
  C = c(svm.bestTune.mccf1$C, svm.bestTune.roc$C)
)

pred.probability <- predict(svm.fit, newdata = data.training, type = "prob")
svm.metrics <- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted)

illustrate.metrics.boxplot(svm.fit$resample %>% filter(degree == hyperparameters.table$degree[1] &
                                                      scale == hyperparameters.table$scale[1] &
                                                      C == hyperparameters.table$C[1]),
                           svm.fit$resample %>% filter(degree == hyperparameters.table$degree[2] &
                                                      scale == hyperparameters.table$scale[2] &
                                                      C == hyperparameters.table$C[2]),
                           "svm")
```

**Performance evaluation on test set**
```{r, model.svm.test}
# MCCF1 variant: prediction scores on test data ("active", "delisted")
pred.test.probability <- predict(svm.fit, newdata = data.test, type = "prob")

# ROC variant: fit model using best hyperparameters and 
#              find best threshold and
#              prediction scores on test data ("active", "delisted")
set.seed(1)
svm.fit.roc <- train(response ~ ., data = data.training %>% select(-permno, -public_date),
                   preProcess = c("center", "scale"),
                   method = "svmPoly",
                   trControl = trainControl(method = "cv", number = 10,
                             classProbs = TRUE, summaryFunction = compute.summary), 
                   tuneGrid = data.frame(
                     degree = svm.bestTune.roc$degree,
                     scale = svm.bestTune.roc$scale,
                     C = svm.bestTune.roc$C
                   ),
                   verbose = TRUE, 
                   metric = "auc.roc") # specify which metric to optimize
svm.metrics.roc.cutpoint <- compute.roc(
  predict(svm.fit.roc, newdata = data.training, type = "prob")$delisted,
  data.training$response)$best_cutpoint
pred.test.probability.roc <- predict(svm.fit.roc, newdata = data.test, type = "prob")

# metrics on test set using best threshold determined based on train data
svm.summary.test <- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted >= svm.metrics.roc.cutpoint),
      compute.metrics(data.test$response, pred.test.probability$delisted >= svm.metrics$cutpoint.mccf1)) %>%
  cbind(splitting = c("ROC", "MCCF1"), .)
pander(svm.summary.test, caption = "Support Vector Machine on test data")
```

<!-- no variable importance measure available -->

**Marginal plots**
```{r, model.svm.marginal}
# marginal plots of features in com.features.fin on test data
for (feature_identifier in com.features.fin$identifier) {
  plot <- hist_delisting_frequency(
      (data.test %>% 
        inner_join(com %>% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))),
      feature_identifier,
      feature_identifier,
      marginal.data = pred.test.probability$delisted
  )
  print(plot)
}
```
