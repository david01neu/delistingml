## Boosting

https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf

Hyperparameters\

* ```n.trees```: number of iterations $T$\

* ```interaction.depth```: depth of each tree $K$\

* ```shrinkage```: shrinkage (or learning rate) parameter $\lambda$\

* ```bag.fraction````: subsampling rate $p$

Initialize $\hat{f}(x)$ to be a constant and compute $\hat{f}(x) = \arg \min_\rho L(y_i,\rho)$\
For $t$ in $1,\dots,T$ do

1. Compute the negative gradient as the working response
$$
z_i = - \left. \frac{\partial}{\partial f(x_i)} L(y_i, f(x_i)) \right|_{f(x_i)=\hat{f}(x_i)} \qquad \forall i \in \{1,\dots,N\}
$$

2. Randomly select $p \cdot N$ cases from the data set.

3. Fit a regression tree with $K$ terminal nodes, $g(x) = E(z|x)$. This tree is fit using only those randomly selected observations from step 2.
3. Compute the optimal terminal node predictions, $\rho_1,\dots,\rho_K$, as
$$
\rho_k = \arg \min_\rho \sum_{x_i \in S_k}^{N} L(y_i, \hat{f}(x_i) + \rho)
$$
where $S_k$ is the set of $x$s that define terminal node $k$. This step uses only the randomly selected observations.
4. Update the $\hat{f}(x)$ as $\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \rho_{k(x)}$ where $k(x)$ indicates the index of the terminal node into which an observation with features $x$ would fall.

**Model fit**
```{r, model.boosting.fit}
if (REDO_MODEL_FITTING) {
  ### control the computational nuances of the train function ####################
  boosting.fitControl <- trainControl(method = "cv",
                             number = 10,
                             ## Estimate class probabilities
                             classProbs = TRUE,
                             ## Evaluate performance using 
                             ## the following function
                             summaryFunction = compute.summary,
                             returnResamp = "all")
  
  ### fit model over different tuning parameters #################################
  set.seed(1)
  boosting.fit <- train(response ~ ., data = data.training %>% select(-permno, -public_date) %>% 
                     mutate(response = as.factor(response)), 
                   preProcess = c("center", "scale"),
                   method = "gbm", 
                   trControl = boosting.fitControl, 
                   tuneGrid = expand.grid(
                        n.trees = c(100, 150, 250, 500),
                        interaction.depth = c(3, 5, 8), # Hands-On Machine Learning with R: https://bradleyboehmke.github.io/HOML/
                        shrinkage = c(0.001, 0.01, 0.1, 0.2),
                        n.minobsinnode = c(5, 10)),
                        # bag.fraction = 0.5 by default
                   verbose = FALSE, 
                   metric = "mcc.mccf1"
                   ) # specify which metric to optimize
  save(boosting.fit, file = "../data/boosting.fit.RData")
} else {
  load("../data/boosting.fit.RData")
}

# effect of hyperparameters
ggplot(boosting.fit) +
  labs(
    title = "Gradient boosting with regression trees as base-learners",
    subtitle = "MCC vs. min. terminal node size & shrinkage parameter alpha",
    x = "Min. terminal node size & shrinkage parameter",
    y = "MCC (10-fold cross-validation)"
  )
```

**Model information and evaluation on training set**
```{r, model.boosting.best_model}
boosting.bestTune.mccf1 <- boosting.fit$resample %>%
  group_by(n.trees, interaction.depth, shrinkage, n.minobsinnode) %>%
  summarize(mccf1 = mean(mcc.mccf1), .groups = "drop") %>%
  slice(which.max(mccf1))

boosting.bestTune.roc <- boosting.fit$resample %>%
  group_by(n.trees, interaction.depth, shrinkage, n.minobsinnode) %>%
  summarize(auc = mean(auc.roc), .groups = "drop") %>%
  slice(which.max(auc))

# Hyperparameters of best models for MCC-F1 and ROC cutpoints
hyperparameters.table <- tibble(
  cutpoint = c("MCCF1", "ROC"),
  n.trees = c(boosting.bestTune.mccf1$n.trees, boosting.bestTune.roc$n.trees),
  interaction.depth = c(boosting.bestTune.mccf1$interaction.depth, boosting.bestTune.roc$interaction.depth),
  shrinkage = c(boosting.bestTune.mccf1$shrinkage, boosting.bestTune.roc$shrinkage),
  n.minobsinnode = c(boosting.bestTune.mccf1$n.minobsinnode, boosting.bestTune.roc$n.minobsinnode)
  
)

pred.probability <- predict(boosting.fit, newdata = data.training, type = "prob")
boosting.metrics <- illustrate.metrics(hyperparameters.table, data.training$response, pred.probability$delisted)

illustrate.metrics.boxplot(boosting.fit$resample %>% filter(n.trees == hyperparameters.table$n.trees[1] &
                                                      interaction.depth == hyperparameters.table$interaction.depth[1] &
                                                      shrinkage == hyperparameters.table$shrinkage[1] &
                                                      n.minobsinnode == hyperparameters.table$n.minobsinnode[1]),
                           boosting.fit$resample %>% filter(n.trees == hyperparameters.table$n.trees[2] &
                                                      interaction.depth == hyperparameters.table$interaction.depth[2] &
                                                      shrinkage == hyperparameters.table$shrinkage[2] &
                                                      n.minobsinnode == hyperparameters.table$n.minobsinnode[2]),
                           "boosting")
```

**Performance evaluation on test set**
```{r, boosting.test, results='hide'}
# MCCF1 variant: prediction scores on test data ("active", "delisted")
pred.test.probability <- predict(boosting.fit, newdata = data.test, type = "prob")

# ROC variant: fit model using best hyperparameters and 
#              find best threshold and
#              prediction scores on test data ("active", "delisted")
set.seed(1)
boosting.fit.roc <- train(response ~ ., data = data.training %>% select(-permno, -public_date),
                   preProcess = c("center", "scale"),
                   method = "gbm", 
                   trControl = trainControl(method = "cv", number = 10,
                             classProbs = TRUE, summaryFunction = compute.summary),
                   tuneGrid = expand.grid(
                        n.trees = boosting.bestTune.roc$n.trees,
                        interaction.depth = boosting.bestTune.roc$interaction.depth,
                        shrinkage = boosting.bestTune.roc$shrinkage,
                        n.minobsinnode = boosting.bestTune.roc$n.minobsinnode),
                   metric = "auc.roc") # specify which metric to optimize
boosting.metrics.roc.cutpoint <- compute.roc(
  predict(boosting.fit.roc, newdata = data.training, type = "prob")$delisted,
  data.training$response)$best_cutpoint
pred.test.probability <- predict(boosting.fit, newdata = data.test, type = "prob")

# metrics on test set using best threshold determined based on train data
boosting.summary.test <- rbind(compute.metrics(data.test$response, pred.test.probability.roc$delisted >= boosting.metrics.roc.cutpoint),
      compute.metrics(data.test$response, pred.test.probability$delisted >= boosting.metrics$cutpoint.mccf1)) %>%
  cbind(splitting = c("ROC", "MCCF1"), .)
pander(boosting.summary.test, caption = "Gradient boosting on test data")
```

**Variable importance**
```{r, model.boosting.vimp}
# returns the reduction attributable to each variable in sum of squared error 
# in predicting the gradient on each iteration. It describes the relative 
# influence  of each variable in reducing the loss function
boosting.vimp.results <- varImp(boosting.fit)$importance
boosting.vimp <- boosting.vimp.results %>%
  mutate(feature = rownames(boosting.vimp.results),
         vimp = Overall) %>% 
  arrange(desc(vimp))

plot.vimp(boosting.vimp, "Importance based on sum of SSE reductions")
```

**Marginal plots**
```{r, model.boosting.marginal}
# marginal plots of 20 most important features on test data
for (feature_identifier in (boosting.vimp %>% filter(row_number() <= 20))$feature) {
  plot <- hist_delisting_frequency(
      (data.test %>% 
        inner_join(com %>% select(permno, public_date, years_to_delisting), by = join_by(permno, public_date))),
      feature_identifier,
      feature_identifier,
      marginal.data = pred.test.probability$delisted
    )
  print(plot)
}
```